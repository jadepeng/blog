<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/jadepeng/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/jadepeng/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/jadepeng/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/jadepeng/images/logo.svg" color="#222">

<link rel="stylesheet" href="/jadepeng/css/main.css">


<link rel="stylesheet" href="/jadepeng/lib/font-awesome/css/all.min.css">
<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blog.iflyresearch.com","root":"/jadepeng/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="文章作者:jqpeng原文链接: K8S集群安装 主要参考 https:&#x2F;&#x2F;github.com&#x2F;opsnull&#x2F;follow-me-install-kubernetes-cluster 01.系统初始化和全局变量添加 k8s 和 docker 账户在每台机器上添加 k8s 账户，可以无密码 sudo： $ sudo useradd -m k8s $ sudo visudo $ sudo grep">
<meta property="og:type" content="article">
<meta property="og:title" content="K8S集群安装">
<meta property="og:url" content="http://blog.iflyresearch.com/2018/11/12/jqpeng-K8S%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/index.html">
<meta property="og:site_name" content="JadePeng的技术笔记本">
<meta property="og:description" content="文章作者:jqpeng原文链接: K8S集群安装 主要参考 https:&#x2F;&#x2F;github.com&#x2F;opsnull&#x2F;follow-me-install-kubernetes-cluster 01.系统初始化和全局变量添加 k8s 和 docker 账户在每台机器上添加 k8s 账户，可以无密码 sudo： $ sudo useradd -m k8s $ sudo visudo $ sudo grep">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://blog.iflyresearch.com/jadepeng/images/cadvisor-home.png">
<meta property="og:image" content="http://blog.iflyresearch.com/jadepeng/images/cadvisor-metrics.png">
<meta property="article:published_time" content="2018-11-12T01:18:00.000Z">
<meta property="article:modified_time" content="2021-07-12T13:28:27.708Z">
<meta property="article:author" content="JadePeng">
<meta property="article:tag" content="jqpeng">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://blog.iflyresearch.com/jadepeng/images/cadvisor-home.png">

<link rel="canonical" href="http://blog.iflyresearch.com/2018/11/12/jqpeng-K8S%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>K8S集群安装 | JadePeng的技术笔记本</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/jadepeng/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">JadePeng的技术笔记本</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">爱学习爱分享</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/jadepeng/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-博客">

    <a href="/jadepeng/categories/%E5%8D%9A%E5%AE%A2/" rel="section"><i class="fa fa-th fa-fw"></i>博客</a>

  </li>
        <li class="menu-item menu-item-分类">

    <a href="/jadepeng/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/jadepeng/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/jadepeng/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://blog.iflyresearch.com/2018/11/12/jqpeng-K8S%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/jadepeng/images/avatar.gif">
      <meta itemprop="name" content="JadePeng">
      <meta itemprop="description" content="JadePeng的技术笔记本">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JadePeng的技术笔记本">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          K8S集群安装
        </h1>

        <div class="post-meta">

         
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-11-12 09:18:00" itemprop="dateCreated datePublished" datetime="2018-11-12T09:18:00+08:00">2018-11-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-07-12 21:28:27" itemprop="dateModified" datetime="2021-07-12T21:28:27+08:00">2021-07-12</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/jadepeng/categories/%E5%8D%9A%E5%AE%A2/" itemprop="url" rel="index"><span itemprop="name">博客</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/jadepeng/categories/%E5%8D%9A%E5%AE%A2/jqpeng/" itemprop="url" rel="index"><span itemprop="name">jqpeng</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>79k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1:11</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>文章作者:jqpeng<br>原文链接: <a target="_blank" rel="noopener" href="https://www.cnblogs.com/xiaoqi/p/9944706.html">K8S集群安装</a></p>
<p>主要参考 <a target="_blank" rel="noopener" href="https://github.com/opsnull/follow-me-install-kubernetes-cluster">https://github.com/opsnull/follow-me-install-kubernetes-cluster</a></p>
<h2 id="01-系统初始化和全局变量"><a href="#01-系统初始化和全局变量" class="headerlink" title="01.系统初始化和全局变量"></a>01.系统初始化和全局变量</h2><h3 id="添加-k8s-和-docker-账户"><a href="#添加-k8s-和-docker-账户" class="headerlink" title="添加 k8s 和 docker 账户"></a>添加 k8s 和 docker 账户</h3><p>在每台机器上添加 k8s 账户，可以无密码 sudo：</p>
<pre><code>$ sudo useradd -m k8s
$ sudo visudo
$ sudo grep &#39;%wheel.*NOPASSWD: ALL&#39; /etc/sudoers
%wheel    ALL=(ALL)    NOPASSWD: ALL
$ sudo gpasswd -a k8s wheel
</code></pre>
<p>在每台机器上添加 docker 账户，将 k8s 账户添加到 docker 组中，同时配置 dockerd 参数：</p>
<pre><code>$ sudo useradd -m docker
$ sudo gpasswd -a k8s docker
$ sudo mkdir -p  /etc/docker/
$ cat /etc/docker/daemon.json
&#123;
    &quot;registry-mirrors&quot;: [&quot;https://hub-mirror.c.163.com&quot;, &quot;https://docker.mirrors.ustc.edu.cn&quot;],
    &quot;max-concurrent-downloads&quot;: 20
&#125;
</code></pre>
<h3 id="无密码-ssh-登录其它节点"><a href="#无密码-ssh-登录其它节点" class="headerlink" title="无密码 ssh 登录其它节点"></a>无密码 ssh 登录其它节点</h3><p>ssh-copy-id root@docker86-18<br> ssh-copy-id root@docker86-21<br> ssh-copy-id root@docker86-91<br> ssh-copy-id root@docker86-9</p>
<p>ssh-copy-id k8s@docker86-155<br> ssh-copy-id k8s@docker86-18<br> ssh-copy-id root@docker86-21<br> ssh-copy-id root@docker86-91<br> ssh-copy-id root@docker86-9</p>
<pre><code>source ./environment.sh
for node_ip in $&#123;NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    ssh root@$&#123;node_ip&#125; &quot;mkdir -p /opt/k8s/bin &amp;&amp; chown -R k8s /opt/k8s &amp;&amp; mkdir -p /etc/kubernetes/cert &amp;&amp;chown -R k8s /etc/kubernetes &amp;&amp; mkdir -p /etc/etcd/cert &amp;&amp; chown -R k8s /etc/etcd/cert &amp;&amp;  mkdir -p /var/lib/etcd &amp;&amp; chown -R k8s /etc/etcd/cert&quot;
    scp environment.sh k8s@$&#123;node_ip&#125;:/opt/k8s/bin/
    ssh k8s@$&#123;node_ip&#125; &quot;chmod +x /opt/k8s/bin/*&quot;
  done
</code></pre>
<h3 id="定义全局变量"><a href="#定义全局变量" class="headerlink" title="定义全局变量"></a>定义全局变量</h3><pre><code>cat &lt;&lt;EOF &gt;environment.sh 
#!/usr/bin/bash

# 生成 EncryptionConfig 所需的加密 key
ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)

# 最好使用 当前未用的网段 来定义服务网段和 Pod 网段

# 服务网段，部署前路由不可达，部署后集群内路由可达(kube-proxy 和 ipvs 保证)
SERVICE_CIDR=&quot;10.69.0.0/16&quot;

# Pod 网段，建议 /16 段地址，部署前路由不可达，部署后集群内路由可达(flanneld 保证)
CLUSTER_CIDR=&quot;170.22.0.0/16&quot;

# 服务端口范围 (NodePort Range)
export NODE_PORT_RANGE=&quot;10000-40000&quot;

# 集群各机器 IP 数组
export NODE_IPS=(192.168.86.154 192.168.86.155 192.168.86.156 192.168.86.18 192.168.86.21 192.168.86.91 192.168.86.9)

# etcd节点
export ETCD_NODE_IPS=(192.168.86.154 192.168.86.155 192.168.86.156)

# 集群各 IP 对应的 主机名数组
export NODE_NAMES=(docker86-154 docker86-155 docker86-156 docker86-18 docker86-21 docker86-91 docker86-9)

# kube-apiserver 的 VIP（HA 组件 keepalived 发布的 IP）
export MASTER_VIP=192.168.86.214

# kube-apiserver VIP 地址（HA 组件 haproxy 监听 8443 端口）
export KUBE_APISERVER=&quot;https://$&#123;MASTER_VIP&#125;:8443&quot;

# HA 节点，配置 VIP 的网络接口名称
export VIP_IF=&quot;em1&quot;

# etcd 集群服务地址列表
export ETCD_ENDPOINTS=&quot;https://192.168.86.154:2379,https://192.168.86.155:2379,https://192.168.86.156:2379&quot;

# etcd 集群间通信的 IP 和端口
export ETCD_NODES=&quot;docker86-154=https://192.168.86.154:2380,docker86-155=https://192.168.86.155:2380,docker86-156=https://192.168.86.156:2380&quot;

# flanneld 网络配置前缀
export FLANNEL_ETCD_PREFIX=&quot;/kubernetes/network&quot;

# kubernetes 服务 IP (一般是 SERVICE_CIDR 中第一个IP)
export CLUSTER_KUBERNETES_SVC_IP=&quot;10.69.0.1&quot;

# 集群 DNS 服务 IP (从 SERVICE_CIDR 中预分配)
export CLUSTER_DNS_SVC_IP=&quot;10.69.0.2&quot;

# 集群 DNS 域名
export CLUSTER_DNS_DOMAIN=&quot;cluster.local.&quot;

# 将二进制目录 /opt/k8s/bin 加到 PATH 中
export PATH=/opt/k8s/bin:$PATH
EOF
</code></pre>
<p>然后，把全局变量定义脚本拷贝到所有节点的 /opt/k8s/bin 目录：</p>
<pre><code>source ./environment.sh
for node_ip in $&#123;NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    scp environment.sh k8s@$&#123;node_ip&#125;:/opt/k8s/bin/
    ssh k8s@$&#123;node_ip&#125; &quot;chmod +x /opt/k8s/bin/*&quot;
  done
</code></pre>
<h3 id="CA证书"><a href="#CA证书" class="headerlink" title="CA证书"></a>CA证书</h3><p>配置文件：<br> 17520h 2年，最大2年</p>
<pre><code>cat &gt; ca-config.json &lt;&lt;EOF
&#123;
  &quot;signing&quot;: &#123;
    &quot;default&quot;: &#123;
      &quot;expiry&quot;: &quot;17520h&quot;
    &#125;,
    &quot;profiles&quot;: &#123;
      &quot;kubernetes&quot;: &#123;
        &quot;usages&quot;: [
            &quot;signing&quot;,
            &quot;key encipherment&quot;,
            &quot;server auth&quot;,
            &quot;client auth&quot;
        ],
        &quot;expiry&quot;: &quot;87600h&quot;
      &#125;
    &#125;
  &#125;
&#125;
EOF
</code></pre>
<p>ca证书签名请求</p>
<pre><code>cat &gt; ca-csr.json &lt;&lt;EOF
&#123;
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;key&quot;: &#123;
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  &#125;,
  &quot;names&quot;: [
    &#123;
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;4Paradigm&quot;
    &#125;
  ]
&#125;
EOF
</code></pre>
<ul>
<li>CN：<code>Common Name</code>，kube-apiserver 从证书中提取该字段作为请求的**用户名 (User Name)**，浏览器使用该字段验证网站是否合法；</li>
<li>O：<code>Organization</code>，kube-apiserver 从证书中提取该字段作为请求用户所属的**组 (Group)**；</li>
<li>kube-apiserver 将提取的 User、Group 作为 <code>RBAC</code> 授权的用户标识；</li>
</ul>
<h4 id="生成-CA-证书和私钥"><a href="#生成-CA-证书和私钥" class="headerlink" title="生成 CA 证书和私钥"></a>生成 CA 证书和私钥</h4><pre><code>cfssl gencert -initca ca-csr.json | cfssljson -bare ca
ls ca*
</code></pre>
<p>将生成的 CA 证书、秘钥文件、配置文件拷贝到所有节点的 /etc/kubernetes/cert 目录下：</p>
<pre><code>source /opt/k8s/bin/environment.sh # 导入 NODE_IPS 环境变量
for node_ip in $&#123;NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    ssh root@$&#123;node_ip&#125; &quot;mkdir -p /etc/kubernetes/cert &amp;&amp; chown -R k8s /etc/kubernetes&quot;
    scp ca*.pem ca-config.json k8s@$&#123;node_ip&#125;:/etc/kubernetes/cert
  done
</code></pre>
<h3 id="客户端安装"><a href="#客户端安装" class="headerlink" title="客户端安装"></a>客户端安装</h3><p>wget <a target="_blank" rel="noopener" href="https://dl.k8s.io/v1.12.1/kubernetes-client-linux-amd64.tar.gz">https://dl.k8s.io/v1.12.1/kubernetes-client-linux-amd64.tar.gz</a><br> tar -xzvf kubernetes-client-linux-amd64.tar.gz</p>
<p>分发到所有使用 kubectl 的节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    scp kubernetes/client/bin/kubectl k8s@$&#123;node_ip&#125;:/opt/k8s/bin/
    ssh k8s@$&#123;node_ip&#125; &quot;chmod +x /opt/k8s/bin/*&quot;
  done
</code></pre>
<h3 id="创建-admin-证书和私钥"><a href="#创建-admin-证书和私钥" class="headerlink" title="创建 admin 证书和私钥"></a>创建 admin 证书和私钥</h3><p>kubectl 与 apiserver https 安全端口通信，apiserver 对提供的证书进行认证和授权。</p>
<p>kubectl 作为集群的管理工具，需要被授予最高权限。这里创建具有最高权限的 admin 证书。</p>
<p>创建证书签名请求：</p>
<pre><code>cat &gt; admin-csr.json &lt;&lt;EOF
&#123;
  &quot;CN&quot;: &quot;admin&quot;,
  &quot;hosts&quot;: [],
  &quot;key&quot;: &#123;
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  &#125;,
  &quot;names&quot;: [
    &#123;
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;system:masters&quot;,
      &quot;OU&quot;: &quot;4Paradigm&quot;
    &#125;
  ]
&#125;
EOF
</code></pre>
<p>O 为 system:masters，kube-apiserver 收到该证书后将请求的 Group 设置为 system:masters；<br> 预定义的 ClusterRoleBinding cluster-admin 将 Group system:masters 与 Role cluster-admin 绑定，该 Role 授予所有 API的权限；<br> 该证书只会被 kubectl 当做 client 证书使用，所以 hosts 字段为空；</p>
<h3 id="生成证书和私钥："><a href="#生成证书和私钥：" class="headerlink" title="生成证书和私钥："></a>生成证书和私钥：</h3><pre><code>cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
  -ca-key=/etc/kubernetes/cert/ca-key.pem \
  -config=/etc/kubernetes/cert/ca-config.json \
  -profile=kubernetes admin-csr.json | cfssljson -bare admin
ls admin*
</code></pre>
<h3 id="创建-kubeconfig-文件"><a href="#创建-kubeconfig-文件" class="headerlink" title="创建 kubeconfig 文件"></a>创建 kubeconfig 文件</h3><p>kubeconfig 为 kubectl 的配置文件，包含访问 apiserver 的所有信息，如 apiserver 地址、CA 证书和自身使用的证书；</p>
<pre><code>source /opt/k8s/bin/environment.sh
# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/cert/ca.pem \
  --embed-certs=true \
  --server=$&#123;KUBE_APISERVER&#125; \
  --kubeconfig=kubectl.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials admin \
  --client-certificate=admin.pem \
  --client-key=admin-key.pem \
  --embed-certs=true \
  --kubeconfig=kubectl.kubeconfig

# 设置上下文参数
kubectl config set-context kubernetes \
  --cluster=kubernetes \
  --user=admin \
  --kubeconfig=kubectl.kubeconfig
  
# 设置默认上下文
kubectl config use-context kubernetes --kubeconfig=kubectl.kubeconfig
--certificate-authority：验证 kube-apiserver 证书的根证书；
--client-certificate、--client-key：刚生成的 admin 证书和私钥，连接 kube-apiserver 时使用；
--embed-certs=true：将 ca.pem 和 admin.pem 证书内容嵌入到生成的 kubectl.kubeconfig 文件中(不加时，写入的是证书文件路径)；
</code></pre>
<h3 id="分发-kubeconfig-文件"><a href="#分发-kubeconfig-文件" class="headerlink" title="分发 kubeconfig 文件"></a>分发 kubeconfig 文件</h3><p>分发到所有使用 kubectl 命令的节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    ssh k8s@$&#123;node_ip&#125; &quot;mkdir -p ~/.kube&quot;
    scp kubectl.kubeconfig k8s@$&#123;node_ip&#125;:~/.kube/config
    ssh root@$&#123;node_ip&#125; &quot;mkdir -p ~/.kube&quot;
    scp kubectl.kubeconfig root@$&#123;node_ip&#125;:~/.kube/config
  done
</code></pre>
<p>保存到用户的 ~/.kube/config 文件；</p>
<h2 id="etcd安装"><a href="#etcd安装" class="headerlink" title="etcd安装"></a>etcd安装</h2><p>到 <a target="_blank" rel="noopener" href="https://github.com/coreos/etcd/releases">https://github.com/coreos/etcd/releases</a> 页面下载最新版本的发布包：</p>
<pre><code>wget https://github.com/etcd-io/etcd/releases/download/v3.3.10/etcd-v3.3.10-linux-amd64.tar.gz
tar -xvf etcd-v3.3.10-linux-amd64.tar.gz
</code></pre>
<h3 id="分发二进制文件到集群所有节点："><a href="#分发二进制文件到集群所有节点：" class="headerlink" title="分发二进制文件到集群所有节点："></a>分发二进制文件到集群所有节点：</h3><pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    scp etcd-v3.3.10-linux-amd64/etcd* k8s@$&#123;node_ip&#125;:/opt/k8s/bin
    ssh k8s@$&#123;node_ip&#125; &quot;chmod +x /opt/k8s/bin/*&quot;
  done
</code></pre>
<h3 id="创建-etcd-证书和私钥"><a href="#创建-etcd-证书和私钥" class="headerlink" title="创建 etcd 证书和私钥"></a>创建 etcd 证书和私钥</h3><p>创建证书签名请求：</p>
<pre><code>cat &gt; etcd-csr.json &lt;&lt;EOF
&#123;
  &quot;CN&quot;: &quot;etcd&quot;,
  &quot;hosts&quot;: [
    &quot;127.0.0.1&quot;,
    &quot;192.168.86.156&quot;,
    &quot;192.168.86.155&quot;,
    &quot;192.168.86.154&quot;
  ],
  &quot;key&quot;: &#123;
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  &#125;,
  &quot;names&quot;: [
    &#123;
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;4Paradigm&quot;
    &#125;
  ]
&#125;
EOF
</code></pre>
<h3 id="生成证书和私钥：-1"><a href="#生成证书和私钥：-1" class="headerlink" title="生成证书和私钥："></a>生成证书和私钥：</h3><pre><code>cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
    -ca-key=/etc/kubernetes/cert/ca-key.pem \
    -config=/etc/kubernetes/cert/ca-config.json \
    -profile=kubernetes etcd-csr.json | cfssljson -bare etcd
ls etcd*
</code></pre>
<h3 id="分发生成的证书和私钥到各-etcd-节点："><a href="#分发生成的证书和私钥到各-etcd-节点：" class="headerlink" title="分发生成的证书和私钥到各 etcd 节点："></a>分发生成的证书和私钥到各 etcd 节点：</h3><pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    ssh root@$&#123;node_ip&#125; &quot;mkdir -p /etc/etcd/cert &amp;&amp; chown -R k8s /etc/etcd/cert&quot;
    scp etcd*.pem k8s@$&#123;node_ip&#125;:/etc/etcd/cert/
  done
</code></pre>
<p>ETCD_NODE_IPS</p>
<h3 id="创建-etcd-的-systemd-unit-模板文件"><a href="#创建-etcd-的-systemd-unit-模板文件" class="headerlink" title="创建 etcd 的 systemd unit 模板文件"></a>创建 etcd 的 systemd unit 模板文件</h3><p>cat &gt; etcd.service.template &lt;&lt;EOF<br> [Unit]<br> Description=Etcd Server<br> After=network.target<br> After=network-online.target<br> Wants=network-online.target<br> Documentation=<a target="_blank" rel="noopener" href="https://github.com/coreos">https://github.com/coreos</a></p>
<p>[Service]<br> User=k8s<br> Type=notify<br> WorkingDirectory=/var/lib/etcd/<br> ExecStart=/opt/k8s/bin/etcd \<br> –data-dir=/var/lib/etcd \<br> –name=##NODE_NAME## \</p>
<p>–cert-file=/etc/etcd/cert/etcd.pem \<br> –key-file=/etc/etcd/cert/etcd-key.pem \<br> –trusted-ca-file=/etc/kubernetes/cert/ca.pem \<br> –peer-cert-file=/etc/etcd/cert/etcd.pem \<br> –peer-key-file=/etc/etcd/cert/etcd-key.pem \<br> –peer-trusted-ca-file=/etc/kubernetes/cert/ca.pem \<br> –peer-client-cert-auth \<br> –client-cert-auth \<br> –listen-peer-urls=https://##NODE_IP##:2380 \<br> –initial-advertise-peer-urls=https://##NODE_IP##:2380 \<br> –listen-client-urls=https://##NODE_IP##:2379,<a target="_blank" rel="noopener" href="http://127.0.0.1:2379/">http://127.0.0.1:2379</a> \<br> –advertise-client-urls=https://##NODE_IP##:2379 \<br> –initial-cluster-token=etcd-cluster-0 \<br> –initial-cluster=${ETCD_NODES} \<br> –initial-cluster-state=new<br> Restart=on-failure<br> RestartSec=5<br> LimitNOFILE=65536</p>
<p>[Install]<br> WantedBy=multi-user.target<br> EOF</p>
<p>User：指定以 k8s 账户运行；<br> WorkingDirectory、–data-dir：指定工作目录和数据目录为 /var/lib/etcd，需在启动服务前创建这个目录；<br> –name：指定节点名称，当 –initial-cluster-state 值为 new 时，–name 的参数值必须位于 –initial-cluster 列表中；<br> –cert-file、–key-file：etcd server 与 client 通信时使用的证书和私钥；<br> –trusted-ca-file：签名 client 证书的 CA 证书，用于验证 client 证书；<br> –peer-cert-file、–peer-key-file：etcd 与 peer 通信使用的证书和私钥；<br> –peer-trusted-ca-file：签名 peer 证书的 CA 证书，用于验证 peer 证书；</p>
<h3 id="为各节点创建和分发-etcd-systemd-unit-文件"><a href="#为各节点创建和分发-etcd-systemd-unit-文件" class="headerlink" title="为各节点创建和分发 etcd systemd unit 文件"></a>为各节点创建和分发 etcd systemd unit 文件</h3><pre><code>source /opt/k8s/bin/environment.sh
for (( i=0; i &lt; 3; i++ ))
  do
    sed -e &quot;s/##NODE_NAME##/$&#123;NODE_NAMES[i]&#125;/&quot; -e &quot;s/##NODE_IP##/$&#123;NODE_IPS[i]&#125;/&quot; etcd.service.template &gt; etcd-$&#123;NODE_IPS[i]&#125;.service 
  done
ls *.service
</code></pre>
<p>分发生成的 systemd unit 文件：</p>
<p>source /opt/k8s/bin/environment.sh<br> for node_ip in ${ETCD_NODE_IPS[@]}<br> do<br> echo “&gt;&gt;&gt; ${node_ip}”<br> ssh root@${node_ip} “mkdir -p /var/lib/etcd &amp;&amp; chown -R k8s /var/lib/etcd”<br> scp etcd-${node_ip}.service root@${node_ip}:/etc/systemd/system/etcd.service<br> done</p>
<h3 id="启动-etcd-服务"><a href="#启动-etcd-服务" class="headerlink" title="启动 etcd 服务"></a>启动 etcd 服务</h3><p>source ./environment.sh<br> for node_ip in ${ETCD_NODE_IPS[@]}<br> do<br> echo “&gt;&gt;&gt; ${node_ip}”<br> ssh root@${node_ip} “systemctl daemon-reload &amp;&amp; systemctl enable etcd &amp;&amp; systemctl restart etcd &amp;”<br> done</p>
<p>etcd 进程首次启动时会等待其它节点的 etcd 加入集群，命令 systemctl start etcd 会卡住一段时间，为正常现象。</p>
<h3 id="检查启动结果"><a href="#检查启动结果" class="headerlink" title="检查启动结果"></a>检查启动结果</h3><pre><code>source ./environment.sh
for node_ip in $&#123;ETCD_NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    ssh k8s@$&#123;node_ip&#125; &quot;systemctl status etcd|grep Active&quot;
  done
</code></pre>
<p>确保状态为 active (running)，否则查看日志，确认原因：</p>
<p>$ journalctl -u etcd</p>
<h3 id="验证服务状态"><a href="#验证服务状态" class="headerlink" title="验证服务状态"></a>验证服务状态</h3><p>部署完 etcd 集群后，在任一 etc 节点上执行如下命令：</p>
<p>source ./environment.sh<br> for node_ip in ${ETCD_NODE_IPS[@]}<br> do<br> echo “&gt;&gt;&gt; ${node_ip}”<br> ETCDCTL_API=3 /opt/k8s/bin/etcdctl<br> –endpoints=https://${node_ip}:2379<br> –cacert=/etc/kubernetes/cert/ca.pem<br> –cert=/etc/etcd/cert/etcd.pem<br> –key=/etc/etcd/cert/etcd-key.pem endpoint health<br> done<br> 预期输出：</p>
<blockquote>
<blockquote>
<blockquote>
<p>192.168.86.154<br><a target="_blank" rel="noopener" href="https://192.168.86.154:2379/">https://192.168.86.154:2379</a> is healthy: successfully committed proposal: took = 2.197007ms</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>192.168.86.155<br><a target="_blank" rel="noopener" href="https://192.168.86.155:2379/">https://192.168.86.155:2379</a> is healthy: successfully committed proposal: took = 2.299328ms</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>192.168.86.156<br><a target="_blank" rel="noopener" href="https://192.168.86.156:2379/">https://192.168.86.156:2379</a> is healthy: successfully committed proposal: took = 2.014274ms</p>
</blockquote>
</blockquote>
</blockquote>
<h2 id="05-部署-flannel-网络"><a href="#05-部署-flannel-网络" class="headerlink" title="05.部署 flannel 网络"></a>05.部署 flannel 网络</h2><p>kubernetes 要求集群内各节点(包括 master 节点)能通过 Pod 网段互联互通。flannel 使用 vxlan 技术为各节点创建一个可以互通的 Pod 网络，使用的端口为 UDP 8472，需要开放该端口（如公有云 AWS 等）。</p>
<p>flannel 第一次启动时，从 etcd 获取 Pod 网段信息，为本节点分配一个未使用的 /24 段地址，然后创建 flannel.1（也可能是其它名称，如 flannel1 等） 接口。</p>
<p>flannel 将分配的 Pod 网段信息写入 /run/flannel/docker 文件，docker 后续使用这个文件中的环境变量设置 docker0 网桥。</p>
<h3 id="下载和分发-flanneld-二进制文件"><a href="#下载和分发-flanneld-二进制文件" class="headerlink" title="下载和分发 flanneld 二进制文件"></a>下载和分发 flanneld 二进制文件</h3><p>到 <a target="_blank" rel="noopener" href="https://github.com/coreos/flannel/releases">https://github.com/coreos/flannel/releases</a> 页面下载最新版本的发布包：</p>
<pre><code>mkdir flannel
wget https://github.com/coreos/flannel/releases/download/v0.10.0/flannel-v0.10.0-linux-amd64.tar.gz
tar -xzvf flannel-v0.10.0-linux-amd64.tar.gz -C flannel
</code></pre>
<h3 id="创建证书签名请求："><a href="#创建证书签名请求：" class="headerlink" title="创建证书签名请求："></a>创建证书签名请求：</h3><pre><code>cat &gt; flanneld-csr.json &lt;&lt;EOF
&#123;
  &quot;CN&quot;: &quot;flanneld&quot;,
  &quot;hosts&quot;: [],
  &quot;key&quot;: &#123;
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  &#125;,
  &quot;names&quot;: [
    &#123;
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;4Paradigm&quot;
    &#125;
  ]
&#125;
EOF
</code></pre>
<p>该证书只会被 kubectl 当做 client 证书使用，所以 hosts 字段为空；<br> 生成证书和私钥：</p>
<pre><code>cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
  -ca-key=/etc/kubernetes/cert/ca-key.pem \
  -config=/etc/kubernetes/cert/ca-config.json \
  -profile=kubernetes flanneld-csr.json | cfssljson -bare flanneld
ls flanneld*pem
</code></pre>
<p>分发 flanneld 二进制文件和flannel 证书、私钥 到集群所有节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    scp  flannel/&#123;flanneld,mk-docker-opts.sh&#125; k8s@$&#123;node_ip&#125;:/opt/k8s/bin/
    ssh k8s@$&#123;node_ip&#125; &quot;chmod +x /opt/k8s/bin/*&quot;
    ssh root@$&#123;node_ip&#125; &quot;mkdir -p /etc/flanneld/cert &amp;&amp; chown -R k8s /etc/flanneld&quot;scp flanneld*.pem k8s@$&#123;node_ip&#125;:/etc/flanneld/cert
  done
</code></pre>
<p>创建<br> flannel 从 etcd 集群存取网段分配信息，而 etcd 集群启用了双向 x509 证书认证，所以需要为 flanneld 生成证书和私钥。</p>
<p>向 etcd 写入集群 Pod 网段信息<br> 注意：本步骤只需执行一次。</p>
<pre><code>source /opt/k8s/bin/environment.sh
etcdctl \
  --endpoints=$&#123;ETCD_ENDPOINTS&#125; \
  --ca-file=/etc/kubernetes/cert/ca.pem \
  --cert-file=/etc/flanneld/cert/flanneld.pem \
  --key-file=/etc/flanneld/cert/flanneld-key.pem \
  set $&#123;FLANNEL_ETCD_PREFIX&#125;/config &#39;&#123;&quot;Network&quot;:&quot;&#39;$&#123;CLUSTER_CIDR&#125;&#39;&quot;, &quot;SubnetLen&quot;: 24, &quot;Backend&quot;: &#123;&quot;Type&quot;: &quot;vxlan&quot;&#125;&#125;&#39;
</code></pre>
<p>flanneld 当前版本 (v0.10.0) 不支持 etcd v3，故使用 etcd v2 API 写入配置 key 和网段数据；<br> 写入的 Pod 网段 ${CLUSTER_CIDR} 必须是 /16 段地址，必须与 kube-controller-manager 的 –cluster-cidr 参数值一致；</p>
<h3 id="创建-flanneld-的-systemd-unit-文件"><a href="#创建-flanneld-的-systemd-unit-文件" class="headerlink" title="创建 flanneld 的 systemd unit 文件"></a>创建 flanneld 的 systemd unit 文件</h3><pre><code>source /opt/k8s/bin/environment.sh
export IFACE=eno1 # 有的为em1，eth0
cat &gt; flanneld.service &lt;&lt; EOF
[Unit]
Description=Flanneld overlay address etcd agent
After=network.target
After=network-online.target
Wants=network-online.target
After=etcd.service
Before=docker.service

[Service]
Type=notify
ExecStart=/opt/k8s/bin/flanneld \\
  -etcd-cafile=/etc/kubernetes/cert/ca.pem \\
  -etcd-certfile=/etc/flanneld/cert/flanneld.pem \\
  -etcd-keyfile=/etc/flanneld/cert/flanneld-key.pem \\
  -etcd-endpoints=$&#123;ETCD_ENDPOINTS&#125; \\
  -etcd-prefix=$&#123;FLANNEL_ETCD_PREFIX&#125; \\
  -iface=$&#123;IFACE&#125;
ExecStartPost=/opt/k8s/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker
Restart=on-failure

[Install]
WantedBy=multi-user.target
RequiredBy=docker.service
EOF
</code></pre>
<p>mk-docker-opts.sh 脚本将分配给 flanneld 的 Pod 子网网段信息写入 /run/flannel/docker 文件，后续 docker 启动时使用这个文件中的环境变量配置 docker0 网桥；<br> flanneld 使用系统缺省路由所在的接口与其它节点通信，对于有多个网络接口（如内网和公网）的节点，可以用 -iface 参数指定通信接口，如上面的 eth0 接口;<br> flanneld 运行时需要 root 权限；<br> 完整 unit 见 flanneld.service</p>
<p>注意：<br> 有的IFACE=eno1，有的为em1，eth，通过ifconfig查看</p>
<h3 id="分发-flanneld-systemd-unit-文件到所有节点"><a href="#分发-flanneld-systemd-unit-文件到所有节点" class="headerlink" title="分发 flanneld systemd unit 文件到所有节点"></a>分发 flanneld systemd unit 文件到所有节点</h3><pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    scp flanneld.service root@$&#123;node_ip&#125;:/etc/systemd/system/
  done
</code></pre>
<h3 id="启动-flanneld-服务"><a href="#启动-flanneld-服务" class="headerlink" title="启动 flanneld 服务"></a>启动 flanneld 服务</h3><pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable flanneld &amp;&amp; systemctl restart flanneld&quot;
  done
</code></pre>
<h3 id="检查启动结果-1"><a href="#检查启动结果-1" class="headerlink" title="检查启动结果"></a>检查启动结果</h3><pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    ssh k8s@$&#123;node_ip&#125; &quot;systemctl status flanneld|grep Active&quot;
  done
</code></pre>
<p>确保状态为 active (running)，否则查看日志，确认原因：</p>
<p>$ journalctl -u flanneld</p>
<h3 id="检查分配给各-flanneld-的-Pod-网段信息"><a href="#检查分配给各-flanneld-的-Pod-网段信息" class="headerlink" title="检查分配给各 flanneld 的 Pod 网段信息"></a>检查分配给各 flanneld 的 Pod 网段信息</h3><p>查看集群 Pod 网段(/16)：</p>
<p>source /opt/k8s/bin/environment.sh<br> etcdctl<br> –endpoints=${ETCD_ENDPOINTS}<br> –ca-file=/etc/kubernetes/cert/ca.pem<br> –cert-file=/etc/flanneld/cert/flanneld.pem<br> –key-file=/etc/flanneld/cert/flanneld-key.pem<br> get ${FLANNEL_ETCD_PREFIX}/config<br> 输出：</p>
<p>{“Network”:”170.22.0.0/16”, “SubnetLen”: 24, “Backend”: {“Type”: “vxlan”}}</p>
<p>查看已分配的 Pod 子网段列表(/24):</p>
<p>source /opt/k8s/bin/environment.sh<br> etcdctl<br> –endpoints=${ETCD_ENDPOINTS}<br> –ca-file=/etc/kubernetes/cert/ca.pem<br> –cert-file=/etc/flanneld/cert/flanneld.pem<br> –key-file=/etc/flanneld/cert/flanneld-key.pem<br> ls ${FLANNEL_ETCD_PREFIX}/subnets<br> 输出：</p>
<p>/kubernetes/network/subnets/170.22.76.0-24<br> /kubernetes/network/subnets/170.22.84.0-24<br> /kubernetes/network/subnets/170.22.45.0-24<br> /kubernetes/network/subnets/170.22.7.0-24<br> /kubernetes/network/subnets/170.22.12.0-24<br> /kubernetes/network/subnets/170.22.78.0-24<br> /kubernetes/network/subnets/170.22.5.0-24</p>
<p>查看某一 Pod 网段对应的节点 IP 和 flannel 接口地址:</p>
<p>source /opt/k8s/bin/environment.sh<br> etcdctl<br> –endpoints=${ETCD_ENDPOINTS}<br> –ca-file=/etc/kubernetes/cert/ca.pem<br> –cert-file=/etc/flanneld/cert/flanneld.pem<br> –key-file=/etc/flanneld/cert/flanneld-key.pem<br> get ${FLANNEL_ETCD_PREFIX}/subnets/170.22.76.0-24<br> 输出：</p>
<p>{“PublicIP”:”192.168.86.156”,”BackendType”:”vxlan”,”BackendData”:{“VtepMAC”:”6a:aa:ca:8a:ac:ed”}}</p>
<p>验证各节点能通过 Pod 网段互通<br> 在各节点上部署 flannel 后，检查是否创建了 flannel 接口(名称可能为 flannel0、flannel.0、flannel.1 等)：</p>
<p>source /opt/k8s/bin/environment.sh<br> for node_ip in ${NODE_IPS[@]}<br> do<br> echo “&gt;&gt;&gt; ${node_ip}”<br> ssh ${node_ip} “/usr/sbin/ip addr show flannel.1|grep -w inet”<br> done<br> 输出：</p>
<p>inet 172.30.81.0/32 scope global flannel.1<br> inet 172.30.29.0/32 scope global flannel.1<br> inet 172.30.39.0/32 scope global flannel.1<br> 在各节点上 ping 所有 flannel 接口 IP，确保能通：</p>
<p>source /opt/k8s/bin/environment.sh<br> for node_ip in ${NODE_IPS[@]}<br> do<br> echo “&gt;&gt;&gt; ${node_ip}”<br> ssh ${node_ip} “ping -c 1 172.30.81.0”<br> ssh ${node_ip} “ping -c 1 172.30.29.0”<br> ssh ${node_ip} “ping -c 1 172.30.39.0”<br> done</p>
<h2 id="06-0-部署-master-节点"><a href="#06-0-部署-master-节点" class="headerlink" title="06-0.部署 master 节点"></a>06-0.部署 master 节点</h2><p>kubernetes master 节点运行如下组件：</p>
<p>kube-apiserver<br> kube-scheduler<br> kube-controller-manager<br> kube-scheduler 和 kube-controller-manager 可以以集群模式运行，通过 leader 选举产生一个工作进程，其它进程处于阻塞模式。</p>
<p>对于 kube-apiserver，可以运行多个实例（本文档是 3 实例），但对其它组件需要提供统一的访问地址，该地址需要高可用。本文档使用 keepalived 和 haproxy 实现 kube-apiserver VIP 高可用和负载均衡。</p>
<h3 id="下载最新版本的二进制文件"><a href="#下载最新版本的二进制文件" class="headerlink" title="下载最新版本的二进制文件"></a>下载最新版本的二进制文件</h3><p>从 CHANGELOG页面 下载 server tarball 文件（需要翻墙）</p>
<pre><code>wget https://dl.k8s.io/v1.12.1/kubernetes-server-linux-amd64.tar.gz
tar -xzvf kubernetes-server-linux-amd64.tar.gz
</code></pre>
<p>将二进制文件拷贝到所有 所有节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    scp kubernetes/server/bin/* k8s@$&#123;node_ip&#125;:/opt/k8s/bin/
    ssh k8s@$&#123;node_ip&#125; &quot;chmod +x /opt/k8s/bin/*&quot;
  done
</code></pre>
<p>如果有老版本运行，先停止：</p>
<pre><code>systemctl stop kubelet.service 
systemctl stop kube-controller-manager.service 
systemctl stop kube-apiserver.service 
systemctl stop kube-proxy.service 
systemctl stop kube-scheduler.service
systemctl stop etcd
systemctl stop 
</code></pre>
<h2 id="06-1-部署高可用组件（keepalived-haproxy"><a href="#06-1-部署高可用组件（keepalived-haproxy" class="headerlink" title="06-1.部署高可用组件（keepalived+haproxy)"></a>06-1.部署高可用组件（keepalived+haproxy)</h2><p>使用 keepalived 和 haproxy 实现 kube-apiserver 高可用的步骤：</p>
<ul>
<li>keepalived 提供 kube-apiserver 对外服务的 VIP；</li>
<li>haproxy 监听 VIP，后端连接所有 kube-apiserver 实例，提供健康检查和负载均衡功能；</li>
<li>运行 keepalived 和 haproxy 的节点称为 LB 节点。由于 keepalived 是一主多备运行模式，故至少两个 LB 节点。</li>
</ul>
<p>本文档复用 master 节点的三台机器，haproxy 监听的端口(8443) 需要与 kube-apiserver 的端口 6443 不同，避免冲突。</p>
<p>keepalived 在运行过程中周期检查本机的 haproxy 进程状态，如果检测到 haproxy 进程异常，则触发重新选主的过程，VIP 将飘移到新选出来的主节点，从而实现 VIP 的高可用。</p>
<p>所有组件（如 kubeclt、apiserver、controller-manager、scheduler 等）都通过 VIP 和 haproxy 监听的 8443 端口访问 kube-apiserver 服务。</p>
<h3 id="安装软件包"><a href="#安装软件包" class="headerlink" title="安装软件包"></a>安装软件包</h3><pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    ssh root@$&#123;node_ip&#125; &quot;yum install -y keepalived haproxy&quot;
  done
</code></pre>
<p>ubuntu机器，apt-get install</p>
<h3 id="配置和下发-haproxy-配置文件"><a href="#配置和下发-haproxy-配置文件" class="headerlink" title="配置和下发 haproxy 配置文件"></a>配置和下发 haproxy 配置文件</h3><p>haproxy 配置文件：</p>
<pre><code>cat &gt; haproxy.cfg &lt;&lt;EOF
 global
     log /dev/log    local0
     log /dev/log    local1 notice
     chroot /var/lib/haproxy
     stats socket /var/run/haproxy-admin.sock mode 660 level admin
     stats timeout 30s
     user haproxy
     group haproxy
     daemon
     nbproc 1
 
 defaults
     log     global
     timeout connect 5000
     timeout client  10m
     timeout server  10m
 
 listen  admin_stats
     bind 0.0.0.0:10080
     mode http
     log 127.0.0.1 local0 err
     stats refresh 30s
     stats uri /status
     stats realm welcome login\ Haproxy
     stats auth admin:123456
     stats hide-version
     stats admin if TRUE
 
 listen kube-master
     bind 0.0.0.0:8443
     mode tcp
     option tcplog
     balance source
     server 192.168.86.154 192.168.86.154:6443 check inter 2000 fall 2 rise 2 weight 1
     server 192.168.86.155 192.168.86.155:6443 check inter 2000 fall 2 rise 2 weight 1
     server 192.168.86.156 192.168.86.156:6443 check inter 2000 fall 2 rise 2 weight 1
EOF
</code></pre>
<ul>
<li>haproxy 在 10080 端口输出 status 信息；</li>
<li>haproxy 监听所有接口的 8443 端口，该端口与环境变量 ${KUBE_APISERVER} 指定的端口必须一致；</li>
<li>server 字段列出所有 kube-apiserver 监听的 IP 和端口；</li>
</ul>
<p>下发 haproxy.cfg 到所有 master 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;ETCD_NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    scp haproxy.cfg root@$&#123;node_ip&#125;:/etc/haproxy
  done
</code></pre>
<p>起 haproxy 服务</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;ETCD_NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    ssh root@$&#123;node_ip&#125; &quot;systemctl restart haproxy&quot;
  done
</code></pre>
<p>检查 haproxy 服务状态</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;ETCD_NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    ssh root@$&#123;node_ip&#125; &quot;systemctl status haproxy|grep Active&quot;
  done
</code></pre>
<p>确保状态为 active (running)，否则查看日志，确认原因：</p>
<blockquote>
<blockquote>
<blockquote>
<p>192.168.86.154<br> Active: active (running) since Tue 2018-11-06 10:48:13 CST; 5s ago</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>192.168.86.155<br> Active: active (running) since Tue 2018-11-06 10:48:14 CST; 5s ago</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>192.168.86.156<br> Active: active (running) since Tue 2018-11-06 10:48:13 CST; 5s ago</p>
</blockquote>
</blockquote>
</blockquote>
<p>journalctl -u haproxy<br> 检查 haproxy 是否监听 8443 端口：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;ETCD_NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    ssh root@$&#123;node_ip&#125; &quot;netstat -lnpt|grep haproxy&quot;
  done
</code></pre>
<p>确保输出类似于:</p>
<p>tcp        0      0 0.0.0.0:8443            0.0.0.0:*               LISTEN      45606/haproxy</p>
<h3 id="配置和下发-keepalived-配置文件"><a href="#配置和下发-keepalived-配置文件" class="headerlink" title="配置和下发 keepalived 配置文件"></a>配置和下发 keepalived 配置文件</h3><p>keepalived 是一主（master）多备（backup）运行模式，故有两种类型的配置文件。master 配置文件只有一份，backup 配置文件视节点数目而定，对于本文档而言，规划如下：</p>
<p>master: 192.168.86.156<br> backup：192.168.86.155，192.168.86.154</p>
<p>master 配置文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
cat  &gt; keepalived-master.conf &lt;&lt;EOF
global_defs &#123;
    router_id lb-master-105
&#125;

vrrp_script check-haproxy &#123;
    script &quot;killall -0 haproxy&quot;
    interval 5
    weight -30
&#125;

vrrp_instance VI-kube-master &#123;
    state MASTER
    priority 120
    dont_track_primary
    interface $&#123;VIP_IF&#125;
    virtual_router_id 68
    advert_int 3
    track_script &#123;
        check-haproxy
    &#125;
    virtual_ipaddress &#123;
        $&#123;MASTER_VIP&#125;
    &#125;
&#125;
EOF
</code></pre>
<p>VIP 所在的接口（interface ${VIP_IF}）为 em1<br> 使用 killall -0 haproxy 命令检查所在节点的 haproxy 进程是否正常。如果异常则将权重减少（-30）,从而触发重新选主过程；<br> router_id、virtual_router_id 用于标识属于该 HA 的 keepalived 实例，如果有多套 keepalived HA，则必须各不相同；<br> backup 配置文件：</p>
<p>source /opt/k8s/bin/environment.sh<br> cat  &gt; keepalived-backup.conf &lt;&lt;EOF<br> global_defs {<br> router_id lb-backup-105<br> }</p>
<p>vrrp_script check-haproxy {<br> script “killall -0 haproxy”<br> interval 5<br> weight -30<br> }</p>
<p>vrrp_instance VI-kube-master {<br> state BACKUP<br> priority 110<br> dont_track_primary<br> interface ${VIP_IF}<br> virtual_router_id 68<br> advert_int 3<br> track_script {<br> check-haproxy<br> }<br> virtual_ipaddress {<br> ${MASTER_VIP}<br> }<br> }<br> EOF</p>
<p>VIP 所在的接口（interface ${VIP_IF}）为 em1<br> 使用 killall -0 haproxy 命令检查所在节点的 haproxy 进程是否正常。如果异常则将权重减少（-30）,从而触发重新选主过程；<br> router_id、virtual_router_id 用于标识属于该 HA 的 keepalived 实例，如果有多套 keepalived HA，则必须各不相同；<br> priority 的值必须小于 master 的值；</p>
<h3 id="下发-keepalived-配置文件"><a href="#下发-keepalived-配置文件" class="headerlink" title="下发 keepalived 配置文件"></a>下发 keepalived 配置文件</h3><p>下发 master 配置文件：</p>
<pre><code>scp keepalived-master.conf root@172.27.129.105:/etc/keepalived/keepalived.conf
</code></pre>
<p>下发 backup 配置文件：</p>
<pre><code>scp keepalived-backup.conf root@172.27.129.111:/etc/keepalived/keepalived.conf
scp keepalived-backup.conf root@172.27.129.112:/etc/keepalived/keepalived.conf
</code></pre>
<p>起 keepalived 服务</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;ETCD_NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    ssh root@$&#123;node_ip&#125; &quot;systemctl restart keepalived&quot;
  done
</code></pre>
<p>检查 keepalived 服务<br> source /opt/k8s/bin/environment.sh<br> for node_ip in ${ETCD_NODE_IPS[@]}<br> do<br> echo “&gt;&gt;&gt; ${node_ip}”<br> ssh root@${node_ip} “systemctl status keepalived|grep Active”<br> done<br> 确保状态为 active (running)，否则查看日志（journalctl -u keepalived），确认原因：</p>
<blockquote>
<blockquote>
<blockquote>
<p>192.168.86.154<br> Active: active (running) since Tue 2018-11-06 10:54:01 CST; 17s ago</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>192.168.86.155<br> Active: active (running) since Tue 2018-11-06 10:54:03 CST; 18s ago</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>192.168.86.156<br> Active: active (running) since Tue 2018-11-06 10:54:03 CST; 17s ago</p>
</blockquote>
</blockquote>
</blockquote>
<p>查看 VIP 所在的节点，确保可以 ping 通 VIP：</p>
<p>source /opt/k8s/bin/environment.sh<br> for node_ip in ${NODE_IPS[@]}<br> do<br> echo “&gt;&gt;&gt; ${node_ip}”<br> ssh ${node_ip} “/usr/sbin/ip addr show ${VIP_IF}”<br> ssh ${node_ip} “ping -c 1 ${MASTER_VIP}”<br> done<br> 查看 haproxy 状态页面<br> 浏览器访问 ${MASTER_VIP}:10080/status 地址，查看 haproxy 状态页面：</p>
<h2 id="06-1-部署-kube-apiserver-组件"><a href="#06-1-部署-kube-apiserver-组件" class="headerlink" title="06-1.部署 kube-apiserver 组件"></a>06-1.部署 kube-apiserver 组件</h2><p>使用 keepalived 和 haproxy 部署一个 3 节点高可用 master 集群的步骤，对应的 LB VIP 为环境变量 ${MASTER_VIP}。</p>
<h3 id="创建-kubernetes-证书和私钥"><a href="#创建-kubernetes-证书和私钥" class="headerlink" title="创建 kubernetes 证书和私钥"></a>创建 kubernetes 证书和私钥</h3><pre><code>source /opt/k8s/bin/environment.sh
cat &gt; kubernetes-csr.json &lt;&lt;EOF
&#123;
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;hosts&quot;: [
    &quot;127.0.0.1&quot;,
    &quot;192.168.86.156&quot;,
    &quot;192.168.86.155&quot;,
    &quot;192.168.86.154&quot;,
    &quot;192.168.86.9&quot;,
    &quot;$&#123;MASTER_VIP&#125;&quot;,
    &quot;$&#123;CLUSTER_KUBERNETES_SVC_IP&#125;&quot;,
    &quot;kubernetes&quot;,
    &quot;kubernetes.default&quot;,
    &quot;kubernetes.default.svc&quot;,
    &quot;kubernetes.default.svc.local&quot;,
    &quot;kubernetes.default.svc.local.com&quot;
  ],
  &quot;key&quot;: &#123;
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  &#125;,
  &quot;names&quot;: [
    &#123;
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;4Paradigm&quot;
    &#125;
  ]
&#125;
EOF
</code></pre>
<ul>
<li>hosts 字段指定授权使用该证书的 IP 或域名列表，这里列出了 VIP 、apiserver 节点 IP、kubernetes 服务 IP 和域名</li>
<li>域名最后字符不能是 .(如不能为 kubernetes.default.svc.cluster.local.)，否则解析时失败，提示： x509: cannot parse dnsName “kubernetes.default.svc.cluster.local.”；</li>
<li>如果使用非 cluster.local 域名，如 opsnull.com，则需要修改域名列表中的最后两个域名为：kubernetes.default.svc.opsnull、kubernetes.default.svc.opsnull.com</li>
<li>kubernetes 服务 IP 是 apiserver 自动创建的，一般是 –service-cluster-ip-range 参数指定的网段的第一个IP，后续可以通过如下命令获取：kubectl get svc kubernetes</li>
</ul>
<p>生成证书和私钥：</p>
<pre><code>cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
  -ca-key=/etc/kubernetes/cert/ca-key.pem \
  -config=/etc/kubernetes/cert/ca-config.json \
  -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes
ls kubernetes*pem
</code></pre>
<p>将生成的证书和私钥文件拷贝到 master 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;ETCD_NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    ssh root@$&#123;node_ip&#125; &quot;mkdir -p /etc/kubernetes/cert/ &amp;&amp; sudo chown -R k8s /etc/kubernetes/cert/&quot;
    scp kubernetes*.pem k8s@$&#123;node_ip&#125;:/etc/kubernetes/cert/
  done
</code></pre>
<p>k8s 账户可以读写 /etc/kubernetes/cert/ 目录；</p>
<h3 id="创建加密配置文件"><a href="#创建加密配置文件" class="headerlink" title="创建加密配置文件"></a>创建加密配置文件</h3><pre><code>source /opt/k8s/bin/environment.sh
cat &gt; encryption-config.yaml &lt;&lt;EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: $&#123;ENCRYPTION_KEY&#125;
      - identity: &#123;&#125;
EOF

将加密配置文件拷贝到 master 节点的 /etc/kubernetes 目录下：

source /opt/k8s/bin/environment.sh
for node_ip in $&#123;ETCD_NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    scp encryption-config.yaml root@$&#123;node_ip&#125;:/etc/kubernetes/
  done
</code></pre>
<h3 id="创建-kube-apiserver-systemd-unit-模板文件"><a href="#创建-kube-apiserver-systemd-unit-模板文件" class="headerlink" title="创建 kube-apiserver systemd unit 模板文件"></a>创建 kube-apiserver systemd unit 模板文件</h3><pre><code>source /opt/k8s/bin/environment.sh
cat &gt; kube-apiserver.service.template &lt;&lt;EOF
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
ExecStart=/opt/k8s/bin/kube-apiserver \\
  --enable-admission-plugins=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\
  --anonymous-auth=false \\
  --experimental-encryption-provider-config=/etc/kubernetes/encryption-config.yaml \\
  --advertise-address=##NODE_IP## \\
  --bind-address=##NODE_IP## \\
  --insecure-port=0 \\
  --authorization-mode=Node,RBAC \\
  --runtime-config=api/all \\
  --enable-bootstrap-token-auth \\
  --service-cluster-ip-range=$&#123;SERVICE_CIDR&#125; \\
  --service-node-port-range=$&#123;NODE_PORT_RANGE&#125; \\
  --tls-cert-file=/etc/kubernetes/cert/kubernetes.pem \\
  --tls-private-key-file=/etc/kubernetes/cert/kubernetes-key.pem \\
  --client-ca-file=/etc/kubernetes/cert/ca.pem \\
  --kubelet-client-certificate=/etc/kubernetes/cert/kubernetes.pem \\
  --kubelet-client-key=/etc/kubernetes/cert/kubernetes-key.pem \\
  --service-account-key-file=/etc/kubernetes/cert/ca-key.pem \\
  --etcd-cafile=/etc/kubernetes/cert/ca.pem \\
  --etcd-certfile=/etc/kubernetes/cert/kubernetes.pem \\
  --etcd-keyfile=/etc/kubernetes/cert/kubernetes-key.pem \\
  --etcd-servers=$&#123;ETCD_ENDPOINTS&#125; \\
  --enable-swagger-ui=true \\
  --allow-privileged=true \\
  --apiserver-count=3 \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-path=/var/log/kube-apiserver-audit.log \\
  --event-ttl=1h \\
  --alsologtostderr=true \\
  --logtostderr=false \\
  --log-dir=/var/log/kubernetes \\
  --v=2
Restart=on-failure
RestartSec=5
Type=notify
User=k8s
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<ul>
<li>–experimental-encryption-provider-config：启用加密特性；</li>
<li>–authorization-mode=Node,RBAC： 开启 Node 和 RBAC 授权模式，拒绝未授权的请求；</li>
<li>–enable-admission-plugins：启用 ServiceAccount 和 NodeRestriction；</li>
<li>–service-account-key-file：签名 ServiceAccount Token 的公钥文件，kube-controller-manager 的 –service-account-private-key-file 指定私钥文件，两者配对使用；</li>
<li>–tls-*-file：指定 apiserver 使用的证书、私钥和 CA 文件。–client-ca-file 用于验证 client (kue-controller-manager、kube-scheduler、kubelet、kube-proxy 等)请求所带的证书；</li>
<li>–kubelet-client-certificate、–kubelet-client-key：如果指定，则使用 https 访问 kubelet APIs；需要为证书对应的用户(上面 kubernetes*.pem 证书的用户为 kubernetes) 用户定义 RBAC 规则，否则访问 kubelet API * 时提示未授权；</li>
<li>–bind-address： 不能为 127.0.0.1，否则外界不能访问它的安全端口 6443；</li>
<li>–insecure-port=0：关闭监听非安全端口(8080)；</li>
<li>–service-cluster-ip-range： 指定 Service Cluster IP 地址段；</li>
<li>–service-node-port-range： 指定 NodePort 的端口范围；</li>
<li>–runtime-config=api/all=true： 启用所有版本的 APIs，如 autoscaling/v2alpha1；</li>
<li>–enable-bootstrap-token-auth：启用 kubelet bootstrap 的 token 认证；</li>
<li>–apiserver-count=3：指定集群运行模式，多台 kube-apiserver 会通过 leader 选举产生一个工作节点，其它节点处于阻塞状态；</li>
<li>User=k8s：使用 k8s 账户运行；</li>
</ul>
<h3 id="为各节点创建和分发-kube-apiserver-systemd-unit-文件"><a href="#为各节点创建和分发-kube-apiserver-systemd-unit-文件" class="headerlink" title="为各节点创建和分发 kube-apiserver systemd unit 文件"></a>为各节点创建和分发 kube-apiserver systemd unit 文件</h3><p>替换模板文件中的变量，为各节点创建 systemd unit 文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for (( i=0; i &lt; 3; i++ ))
  do
    sed -e &quot;s/##NODE_NAME##/$&#123;NODE_NAMES[i]&#125;/&quot; -e &quot;s/##NODE_IP##/$&#123;NODE_IPS[i]&#125;/&quot; kube-apiserver.service.template &gt; kube-apiserver-$&#123;NODE_IPS[i]&#125;.service 
  done
ls kube-apiserver*.service
</code></pre>
<p>分发生成的 systemd unit 文件</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;ETCD_NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    ssh root@$&#123;node_ip&#125; &quot;mkdir -p /var/log/kubernetes &amp;&amp; chown -R k8s /var/log/kubernetes&quot;
    scp kube-apiserver-$&#123;node_ip&#125;.service root@$&#123;node_ip&#125;:/etc/systemd/system/kube-apiserver.service
  done
</code></pre>
<h3 id="启动-kube-apiserver-服务"><a href="#启动-kube-apiserver-服务" class="headerlink" title="启动 kube-apiserver 服务"></a>启动 kube-apiserver 服务</h3><p>source /opt/k8s/bin/environment.sh<br> for node_ip in ${ETCD_NODE_IPS[@]}<br> do<br> echo “&gt;&gt;&gt; ${node_ip}”<br> ssh root@${node_ip} “systemctl daemon-reload &amp;&amp; systemctl enable kube-apiserver &amp;&amp; systemctl restart kube-apiserver”<br> done</p>
<h3 id="检查-kube-apiserver-运行状态"><a href="#检查-kube-apiserver-运行状态" class="headerlink" title="检查 kube-apiserver 运行状态"></a>检查 kube-apiserver 运行状态</h3><pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;ETCD_NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    ssh root@$&#123;node_ip&#125; &quot;systemctl status kube-apiserver |grep &#39;Active:&#39;&quot;
  done
</code></pre>
<p>确保状态为 active (running)，否则到 master 节点查看日志，确认原因：</p>
<p>journalctl -u kube-apiserver</p>
<h3 id="打印-kube-apiserver-写入-etcd-的数据"><a href="#打印-kube-apiserver-写入-etcd-的数据" class="headerlink" title="打印 kube-apiserver 写入 etcd 的数据"></a>打印 kube-apiserver 写入 etcd 的数据</h3><p>source /opt/k8s/bin/environment.sh<br> ETCDCTL_API=3 etcdctl<br> –endpoints=${ETCD_ENDPOINTS}<br> –cacert=/etc/kubernetes/cert/ca.pem<br> –cert=/etc/etcd/cert/etcd.pem<br> –key=/etc/etcd/cert/etcd-key.pem<br> get /registry/ –prefix –keys-only</p>
<p>检查集群信息</p>
<pre><code>kubectl cluster-info
kubectl get all --all-namespaces
kubectl get componentstatuses
</code></pre>
<p>检查 kube-apiserver 监听的端口<br> sudo netstat -lnpt|grep kube<br> tcp        0      0 172.27.129.105:6443     0.0.0.0:*               LISTEN      13075/kube-apiserve</p>
<p>6443: 接收 https 请求的安全端口，对所有请求做认证和授权；<br> 由于关闭了非安全端口，故没有监听 8080；</p>
<h3 id="授予-kubernetes-证书访问-kubelet-API-的权限"><a href="#授予-kubernetes-证书访问-kubelet-API-的权限" class="headerlink" title="授予 kubernetes 证书访问 kubelet API 的权限"></a>授予 kubernetes 证书访问 kubelet API 的权限</h3><p>在执行 kubectl exec、run、logs 等命令时，apiserver 会转发到 kubelet。这里定义 RBAC 规则，授权 apiserver 调用 kubelet API。</p>
<pre><code>kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes
</code></pre>
<h2 id="06-3-部署高可用-kube-controller-manager-集群"><a href="#06-3-部署高可用-kube-controller-manager-集群" class="headerlink" title="06-3.部署高可用 kube-controller-manager 集群"></a>06-3.部署高可用 kube-controller-manager 集群</h2><p>该集群包含 3 个节点，启动后将通过竞争选举机制产生一个 leader 节点，其它节点为阻塞状态。当 leader 节点不可用后，剩余节点将再次进行选举产生新的 leader 节点，从而保证服务的可用性。</p>
<p>为保证通信安全，本文档先生成 x509 证书和私钥，kube-controller-manager 在如下两种情况下使用该证书：</p>
<p>与 kube-apiserver 的安全端口通信时;<br> 在安全端口(https，10252) 输出 prometheus 格式的 metrics；</p>
<h3 id="创建-kube-controller-manager-证书和私钥"><a href="#创建-kube-controller-manager-证书和私钥" class="headerlink" title="创建 kube-controller-manager 证书和私钥"></a>创建 kube-controller-manager 证书和私钥</h3><p>创建证书签名请求：</p>
<pre><code>cat &gt; kube-controller-manager-csr.json &lt;&lt;EOF
&#123;
    &quot;CN&quot;: &quot;system:kube-controller-manager&quot;,
    &quot;key&quot;: &#123;
        &quot;algo&quot;: &quot;rsa&quot;,
        &quot;size&quot;: 2048
    &#125;,
    &quot;hosts&quot;: [
      &quot;127.0.0.1&quot;,
      &quot;192.168.86.156&quot;,  &quot;192.168.86.155&quot;,  &quot;192.168.86.154&quot;
    ],
    &quot;names&quot;: [
      &#123;
        &quot;C&quot;: &quot;CN&quot;,
        &quot;ST&quot;: &quot;BeiJing&quot;,
        &quot;L&quot;: &quot;BeiJing&quot;,
        &quot;O&quot;: &quot;system:kube-controller-manager&quot;,
        &quot;OU&quot;: &quot;4Paradigm&quot;
      &#125;
    ]
&#125;
EOF
</code></pre>
<p>hosts 列表包含所有 kube-controller-manager 节点 IP；<br> CN 为 system:kube-controller-manager、O 为 system:kube-controller-manager，kubernetes 内置的 ClusterRoleBindings system:kube-controller-manager 赋予 kube-controller-manager 工作所需的权限。<br> 生成证书和私钥：</p>
<pre><code>cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
  -ca-key=/etc/kubernetes/cert/ca-key.pem \
  -config=/etc/kubernetes/cert/ca-config.json \
  -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager
</code></pre>
<p>将生成的证书和私钥分发到所有 master 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;ETCD_NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    scp kube-controller-manager*.pem k8s@$&#123;node_ip&#125;:/etc/kubernetes/cert/
  done
</code></pre>
<h3 id="创建和分发-kubeconfig-文件"><a href="#创建和分发-kubeconfig-文件" class="headerlink" title="创建和分发 kubeconfig 文件"></a>创建和分发 kubeconfig 文件</h3><p>kubeconfig 文件包含访问 apiserver 的所有信息，如 apiserver 地址、CA 证书和自身使用的证书；</p>
<pre><code>source /opt/k8s/bin/environment.sh
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/cert/ca.pem \
  --embed-certs=true \
  --server=$&#123;KUBE_APISERVER&#125; \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-credentials system:kube-controller-manager \
  --client-certificate=kube-controller-manager.pem \
  --client-key=kube-controller-manager-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-context system:kube-controller-manager \
  --cluster=kubernetes \
  --user=system:kube-controller-manager \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config use-context system:kube-controller-manager --kubeconfig=kube-controller-manager.kubeconfig
</code></pre>
<p>分发 kubeconfig 到所有 master 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;ETCD_NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    scp kube-controller-manager.kubeconfig k8s@$&#123;node_ip&#125;:/etc/kubernetes/
  done
</code></pre>
<h3 id="创建和分发-kube-controller-manager-systemd-unit-文件"><a href="#创建和分发-kube-controller-manager-systemd-unit-文件" class="headerlink" title="创建和分发 kube-controller-manager systemd unit 文件"></a>创建和分发 kube-controller-manager systemd unit 文件</h3><pre><code>source /opt/k8s/bin/environment.sh
cat &gt; kube-controller-manager.service &lt;&lt;EOF
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/opt/k8s/bin/kube-controller-manager \\
  --port=0 \\
  --secure-port=10252 \\
  --bind-address=127.0.0.1 \\
  --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\
  --service-cluster-ip-range=$&#123;SERVICE_CIDR&#125; \\
  --cluster-name=kubernetes \\
  --cluster-signing-cert-file=/etc/kubernetes/cert/ca.pem \\
  --cluster-signing-key-file=/etc/kubernetes/cert/ca-key.pem \\
  --experimental-cluster-signing-duration=17520h \\
  --root-ca-file=/etc/kubernetes/cert/ca.pem \\
  --service-account-private-key-file=/etc/kubernetes/cert/ca-key.pem \\
  --leader-elect=true \\
  --feature-gates=RotateKubeletServerCertificate=true \\
  --controllers=*,bootstrapsigner,tokencleaner \\
  --horizontal-pod-autoscaler-use-rest-clients=true \\
  --horizontal-pod-autoscaler-sync-period=10s \\
  --tls-cert-file=/etc/kubernetes/cert/kube-controller-manager.pem \\
  --tls-private-key-file=/etc/kubernetes/cert/kube-controller-manager-key.pem \\
  --use-service-account-credentials=true \\
  --alsologtostderr=true \\
  --logtostderr=false \\
  --log-dir=/var/log/kubernetes \\
  --v=2
Restart=on
Restart=on-failure
RestartSec=5
User=k8s

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<ul>
<li>–port=0：关闭监听 http /metrics 的请求，同时 –address 参数无效，–bind-address 参数有效；</li>
<li>–secure-port=10252、–bind-address=0.0.0.0: 在所有网络接口监听 10252 端口的 https /metrics 请求；</li>
<li>–kubeconfig：指定 kubeconfig 文件路径，kube-controller-manager 使用它连接和验证 kube-apiserver；</li>
<li>–cluster-signing-*-file：签名 TLS Bootstrap 创建的证书；</li>
<li>–experimental-cluster-signing-duration：指定 TLS Bootstrap 证书的有效期；</li>
<li>–root-ca-file：放置到容器 ServiceAccount 中的 CA 证书，用来对 kube-apiserver 的证书进行校验；</li>
<li>–service-account-private-key-file：签名 ServiceAccount 中 Token 的私钥文件，必须和 kube-apiserver 的 –service-account-key-file 指定的公钥文件配对使用；</li>
<li>–service-cluster-ip-range ：指定 Service Cluster IP 网段，必须和 kube-apiserver 中的同名参数一致；</li>
<li>–leader-elect=true：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态；</li>
<li>–feature-gates=RotateKubeletServerCertificate=true：开启 kublet server 证书的自动更新特性；</li>
<li>–controllers=*,bootstrapsigner,tokencleaner：启用的控制器列表，tokencleaner 用于自动清理过期的 Bootstrap token；</li>
<li>–horizontal-pod-autoscaler-*：custom metrics 相关参数，支持 autoscaling/v2alpha1；</li>
<li>–tls-cert-file、–tls-private-key-file：使用 https 输出 metrics 时使用的 Server 证书和秘钥；</li>
<li>–use-service-account-credentials=true:</li>
<li>User=k8s：使用 k8s 账户运行；</li>
<li>kube-controller-manager 不对请求 https metrics 的 Client 证书进行校验，故不需要指定 –tls-ca-file 参数，而且该参数已被淘汰。</li>
</ul>
<p>分发 systemd unit 文件到所有 master 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;ETCD_NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    scp kube-controller-manager.service root@$&#123;node_ip&#125;:/etc/systemd/system/
  done
</code></pre>
<h3 id="kube-controller-manager-的权限"><a href="#kube-controller-manager-的权限" class="headerlink" title="kube-controller-manager 的权限"></a>kube-controller-manager 的权限</h3><p>ClusteRole: system:kube-controller-manager 的权限很小，只能创建 secret、serviceaccount 等资源对象，各 controller 的权限分散到 ClusterRole system:controller:XXX 中。</p>
<p>需要在 kube-controller-manager 的启动参数中添加 –use-service-account-credentials=true 参数，这样 main controller 会为各 controller 创建对应的 ServiceAccount XXX-controller。</p>
<p>内置的 ClusterRoleBinding system:controller:XXX 将赋予各 XXX-controller ServiceAccount 对应的 ClusterRole system:controller:XXX 权限。</p>
<h3 id="启动-kube-controller-manager-服务"><a href="#启动-kube-controller-manager-服务" class="headerlink" title="启动 kube-controller-manager 服务"></a>启动 kube-controller-manager 服务</h3><pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;ETCD_NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    ssh root@$&#123;node_ip&#125; &quot;mkdir -p /var/log/kubernetes &amp;&amp; chown -R k8s /var/log/kubernetes&quot;
    ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-controller-manager &amp;&amp; systemctl restart kube-controller-manager&quot;
  done
</code></pre>
<p>必须先创建日志目录；</p>
<h3 id="检查服务运行状态"><a href="#检查服务运行状态" class="headerlink" title="检查服务运行状态"></a>检查服务运行状态</h3><p>source /opt/k8s/bin/environment.sh<br> for node_ip in ${ETCD_NODE_IPS[@]}<br> do<br> echo “&gt;&gt;&gt; ${node_ip}”<br> ssh k8s@${node_ip} “systemctl status kube-controller-manager|grep Active”<br> done<br> 确保状态为 active (running)，否则查看日志，确认原因：</p>
<pre><code>$ journalctl -u kube-controller-manager
</code></pre>
<h3 id="查看输出的-metric"><a href="#查看输出的-metric" class="headerlink" title="查看输出的 metric"></a>查看输出的 metric</h3><p>注意：以下命令在 kube-controller-manager 节点上执行。</p>
<p>kube-controller-manager 监听 10252 端口，接收 https 请求：</p>
<pre><code>$ sudo netstat -lnpt|grep kube-controll
tcp        0      0 127.0.0.1:10252         0.0.0.0:*               LISTEN      18377/kube-controll
$ curl -s --cacert /etc/kubernetes/cert/ca.pem https://127.0.0.1:10252/metrics |head
# HELP ClusterRoleAggregator_adds Total number of adds handled by workqueue: ClusterRoleAggregator
# TYPE ClusterRoleAggregator_adds counter
ClusterRoleAggregator_adds 3
# HELP ClusterRoleAggregator_depth Current depth of workqueue: ClusterRoleAggregator
# TYPE ClusterRoleAggregator_depth gauge
ClusterRoleAggregator_depth 0
# HELP ClusterRoleAggregator_queue_latency How long an item stays in workqueueClusterRoleAggregator before being requested.
# TYPE ClusterRoleAggregator_queue_latency summary
ClusterRoleAggregator_queue_latency&#123;quantile=&quot;0.5&quot;&#125; 57018
ClusterRoleAggregator_queue_latency&#123;quantile=&quot;0.9&quot;&#125; 57268
</code></pre>
<p>curl –cacert CA 证书用来验证 kube-controller-manager https server 证书；<br> 测试 kube-controller-manager 集群的高可用<br> 停掉一个或两个节点的 kube-controller-manager 服务，观察其它节点的日志，看是否获取了 leader 权限。</p>
<h3 id="查看当前的-leader"><a href="#查看当前的-leader" class="headerlink" title="查看当前的 leader"></a>查看当前的 leader</h3><pre><code>$ kubectl get endpoints kube-controller-manager --namespace=kube-system  -o yaml
apiVersion: v1
kind: Endpoints
metadata:
  annotations:
    control-plane.alpha.kubernetes.io/leader: &#39;&#123;&quot;holderIdentity&quot;:&quot;docker86-155_32dbaca9-e15f-11e8-87e7-e0db5521eb14&quot;,&quot;leaseDurationSeconds&quot;:15,&quot;acquireTime&quot;:&quot;2018-11-06T00:59:52Z&quot;,&quot;renewTime&quot;:&quot;2018-11-06T01:34:01Z&quot;,&quot;leaderTransitions&quot;:39&#125;&#39;
  creationTimestamp: 2018-10-10T15:18:11Z
  name: kube-controller-manager
  namespace: kube-system
  resourceVersion: &quot;6281708&quot;
  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager
  uid: b38d3ea9-cc9f-11e8-9cde-d4ae52a3b675
</code></pre>
<p>可见，当前的 leader 为docker86-155 节点。</p>
<p>参考<br> 关于 controller 权限和 use-service-account-credentials 参数：<a target="_blank" rel="noopener" href="https://github.com/kubernetes/kubernetes/issues/48208">https://github.com/kubernetes/kubernetes/issues/48208</a><br> kublet 认证和授权：<a target="_blank" rel="noopener" href="https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet-authorization">https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet-authorization</a></p>
<h2 id="06-3-部署高可用-kube-scheduler-集群"><a href="#06-3-部署高可用-kube-scheduler-集群" class="headerlink" title="06-3.部署高可用 kube-scheduler 集群"></a>06-3.部署高可用 kube-scheduler 集群</h2><p>该集群包含 3 个节点，启动后将通过竞争选举机制产生一个 leader 节点，其它节点为阻塞状态。当 leader 节点不可用后，剩余节点将再次进行选举产生新的 leader 节点，从而保证服务的可用性。</p>
<p>为保证通信安全，本文档先生成 x509 证书和私钥，kube-scheduler 在如下两种情况下使用该证书：</p>
<p>与 kube-apiserver 的安全端口通信;<br> 在安全端口(https，10251) 输出 prometheus 格式的 metrics；</p>
<h3 id="创建-kube-scheduler-证书和私钥"><a href="#创建-kube-scheduler-证书和私钥" class="headerlink" title="创建 kube-scheduler 证书和私钥"></a>创建 kube-scheduler 证书和私钥</h3><pre><code>cat &gt; kube-scheduler-csr.json &lt;&lt;EOF
&#123;
    &quot;CN&quot;: &quot;system:kube-scheduler&quot;,
    &quot;hosts&quot;: [
      &quot;127.0.0.1&quot;,
      &quot;192.168.86.156&quot;,  &quot;192.168.86.155&quot;,  &quot;192.168.86.154&quot;
    ],
    &quot;key&quot;: &#123;
        &quot;algo&quot;: &quot;rsa&quot;,
        &quot;size&quot;: 2048
    &#125;,
    &quot;names&quot;: [
      &#123;
        &quot;C&quot;: &quot;CN&quot;,
        &quot;ST&quot;: &quot;BeiJing&quot;,
        &quot;L&quot;: &quot;BeiJing&quot;,
        &quot;O&quot;: &quot;system:kube-scheduler&quot;,
        &quot;OU&quot;: &quot;4Paradigm&quot;
      &#125;
    ]
&#125;
EOF
</code></pre>
<p>生成证书和私钥：</p>
<pre><code>cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
  -ca-key=/etc/kubernetes/cert/ca-key.pem \
  -config=/etc/kubernetes/cert/ca-config.json \
  -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler
</code></pre>
<h3 id="创建和分发-kubeconfig-文件-1"><a href="#创建和分发-kubeconfig-文件-1" class="headerlink" title="创建和分发 kubeconfig 文件"></a>创建和分发 kubeconfig 文件</h3><p>kubeconfig 文件包含访问 apiserver 的所有信息，如 apiserver 地址、CA 证书和自身使用的证书；</p>
<pre><code>source /opt/k8s/bin/environment.sh
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/cert/ca.pem \
  --embed-certs=true \
  --server=$&#123;KUBE_APISERVER&#125; \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-credentials system:kube-scheduler \
  --client-certificate=kube-scheduler.pem \
  --client-key=kube-scheduler-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-context system:kube-scheduler \
  --cluster=kubernetes \
  --user=system:kube-scheduler \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config use-context system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig
</code></pre>
<p>分发 kubeconfig 到所有 master 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;ETCD_NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    scp kube-scheduler.kubeconfig k8s@$&#123;node_ip&#125;:/etc/kubernetes/
  done
</code></pre>
<h3 id="创建和分发-kube-scheduler-systemd-unit-文件"><a href="#创建和分发-kube-scheduler-systemd-unit-文件" class="headerlink" title="创建和分发 kube-scheduler systemd unit 文件"></a>创建和分发 kube-scheduler systemd unit 文件</h3><pre><code>cat &gt; kube-scheduler.service &lt;&lt;EOF
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/opt/k8s/bin/kube-scheduler \\
  --address=127.0.0.1 \\
  --kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\
  --leader-elect=true \\
  --alsologtostderr=true \\
  --logtostderr=false \\
  --log-dir=/var/log/kubernetes \\
  --v=2
Restart=on-failure
RestartSec=5
User=k8s

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<p>–address：在 127.0.0.1:10251 端口接收 http /metrics 请求；kube-scheduler 目前还不支持接收 https 请求；<br> –kubeconfig：指定 kubeconfig 文件路径，kube-scheduler 使用它连接和验证 kube-apiserver；<br> –leader-elect=true：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态；<br> User=k8s：使用 k8s 账户运行；</p>
<h3 id="分发-systemd-unit-文件到所有-master-节点："><a href="#分发-systemd-unit-文件到所有-master-节点：" class="headerlink" title="分发 systemd unit 文件到所有 master 节点："></a>分发 systemd unit 文件到所有 master 节点：</h3><pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;ETCD_NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    scp kube-scheduler.service root@$&#123;node_ip&#125;:/etc/systemd/system/
  done
</code></pre>
<h3 id="启动-kube-scheduler-服务"><a href="#启动-kube-scheduler-服务" class="headerlink" title="启动 kube-scheduler 服务"></a>启动 kube-scheduler 服务</h3><pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;ETCD_NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    ssh root@$&#123;node_ip&#125; &quot;mkdir -p /var/log/kubernetes &amp;&amp; chown -R k8s /var/log/kubernetes&quot;
    ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-scheduler &amp;&amp; systemctl restart kube-scheduler&quot;
  done
</code></pre>
<p>必须先创建日志目录；</p>
<p>检查服务运行状态<br> source /opt/k8s/bin/environment.sh<br> for node_ip in ${ETCD_NODE_IPS[@]}<br> do<br> echo “&gt;&gt;&gt; ${node_ip}”<br> ssh k8s@${node_ip} “systemctl status kube-scheduler|grep Active”<br> done</p>
<p>确保状态为 active (running)，否则查看日志，确认原因：</p>
<pre><code>journalctl -u kube-scheduler
</code></pre>
<h3 id="查看输出的-metric-1"><a href="#查看输出的-metric-1" class="headerlink" title="查看输出的 metric"></a>查看输出的 metric</h3><p>注意：以下命令在 kube-scheduler 节点上执行。</p>
<p>kube-scheduler 监听 10251 端口，接收 http 请求：</p>
<p>$ sudo netstat -lnpt|grep kube-sche<br> tcp        0      0 127.0.0.1:10251         0.0.0.0:*               LISTEN      23783/kube-schedule<br> $ curl -s <a target="_blank" rel="noopener" href="http://127.0.0.1:10251/metrics">http://127.0.0.1:10251/metrics</a> |head</p>
<h1 id="HELP-apiserver-audit-event-total-Counter-of-audit-events-generated-and-sent-to-the-audit-backend"><a href="#HELP-apiserver-audit-event-total-Counter-of-audit-events-generated-and-sent-to-the-audit-backend" class="headerlink" title="HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend."></a>HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.</h1><h1 id="TYPE-apiserver-audit-event-total-counter"><a href="#TYPE-apiserver-audit-event-total-counter" class="headerlink" title="TYPE apiserver_audit_event_total counter"></a>TYPE apiserver_audit_event_total counter</h1><p>apiserver_audit_event_total 0</p>
<h1 id="HELP-go-gc-duration-seconds-A-summary-of-the-GC-invocation-durations"><a href="#HELP-go-gc-duration-seconds-A-summary-of-the-GC-invocation-durations" class="headerlink" title="HELP go_gc_duration_seconds A summary of the GC invocation durations."></a>HELP go_gc_duration_seconds A summary of the GC invocation durations.</h1><h1 id="TYPE-go-gc-duration-seconds-summary"><a href="#TYPE-go-gc-duration-seconds-summary" class="headerlink" title="TYPE go_gc_duration_seconds summary"></a>TYPE go_gc_duration_seconds summary</h1><p>go_gc_duration_seconds{quantile=”0”} 9.7715e-05<br> go_gc_duration_seconds{quantile=”0.25”} 0.000107676<br> go_gc_duration_seconds{quantile=”0.5”} 0.00017868<br> go_gc_duration_seconds{quantile=”0.75”} 0.000262444<br> go_gc_duration_seconds{quantile=”1”} 0.001205223</p>
<h3 id="测试-kube-scheduler-集群的高可用"><a href="#测试-kube-scheduler-集群的高可用" class="headerlink" title="测试 kube-scheduler 集群的高可用"></a>测试 kube-scheduler 集群的高可用</h3><p>随便找一个或两个 master 节点，停掉 kube-scheduler 服务，看其它节点是否获取了 leader 权限（systemd 日志）。</p>
<p>查看当前的 leader<br> $ kubectl get endpoints kube-scheduler –namespace=kube-system  -o yaml<br> apiVersion: v1<br> kind: Endpoints<br> metadata:<br> annotations:<br> control-plane.alpha.kubernetes.io/leader: ‘{“holderIdentity”:”kube-node3_61f34593-6cc8-11e8-8af7-5254002f288e”,”leaseDurationSeconds”:15,”acquireTime”:”2018-06-10T16:09:56Z”,”renewTime”:”2018-06-10T16:20:54Z”,”leaderTransitions”:1}’<br> creationTimestamp: 2018-06-10T16:07:33Z<br> name: kube-scheduler<br> namespace: kube-system<br> resourceVersion: “4645”<br> selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler<br> uid: 62382d98-6cc8-11e8-96fa-525400ba84c6</p>
<h2 id="07-1-部署-docker-组件"><a href="#07-1-部署-docker-组件" class="headerlink" title="07-1.部署 docker 组件"></a>07-1.部署 docker 组件</h2><p>docker 是容器的运行环境，管理它的生命周期。kubelet 通过 Container Runtime Interface (CRI) 与 docker 进行交互。</p>
<h3 id="安装依赖包"><a href="#安装依赖包" class="headerlink" title="安装依赖包"></a>安装依赖包</h3><p>参考 <a href="07-0.%E9%83%A8%E7%BD%B2worker%E8%8A%82%E7%82%B9.md">07-0.部署worker节点.md</a></p>
<h3 id="下载和分发-docker-二进制文件"><a href="#下载和分发-docker-二进制文件" class="headerlink" title="下载和分发 docker 二进制文件"></a>下载和分发 docker 二进制文件</h3><p>到 <a target="_blank" rel="noopener" href="http://mirrors.ustc.edu.cn/docker-ce/linux/static/stable/x86_64/">http://mirrors.ustc.edu.cn/docker-ce/linux/static/stable/x86_64/</a> 页面下载最新发布包：</p>
<pre><code>wget http://mirrors.ustc.edu.cn/docker-ce/linux/static/stable/x86_64/docker-18.06.1-ce.tgz
tar -xvf docker-18.06.1-ce.tgz
</code></pre>
<p>分发二进制文件到所有 worker 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    scp docker/docker*  k8s@$&#123;node_ip&#125;:/opt/k8s/bin/
    ssh k8s@$&#123;node_ip&#125; &quot;chmod +x /opt/k8s/bin/*&quot;
  done
</code></pre>
<h3 id="创建和分发-systemd-unit-文件"><a href="#创建和分发-systemd-unit-文件" class="headerlink" title="创建和分发 systemd unit 文件"></a>创建和分发 systemd unit 文件</h3><pre><code>cat &gt; docker.service &lt;&lt;&quot;EOF&quot;
[Unit]
Description=Docker Application Container Engine
Documentation=http://docs.docker.io

[Service]
Environment=&quot;PATH=/opt/k8s/bin:/bin:/sbin:/usr/bin:/usr/sbin&quot;
EnvironmentFile=-/run/flannel/docker
ExecStart=/opt/k8s/bin/dockerd --log-level=error $DOCKER_NETWORK_OPTIONS
ExecReload=/bin/kill -s HUP $MAINPID
Restart=on-failure
RestartSec=5
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
Delegate=yes
KillMode=process

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<ul>
<li>EOF 前后有双引号，这样 bash 不会替换文档中的变量，如 $DOCKER_NETWORK_OPTIONS；</li>
<li>dockerd 运行时会调用其它 docker 命令，如 docker-proxy，所以需要将 docker 命令所在的目录加到 PATH 环境变量中；</li>
<li>flanneld 启动时将网络配置写入 <code>/run/flannel/docker</code> 文件中，dockerd 启动前读取该文件中的环境变量 <code>DOCKER_NETWORK_OPTIONS</code> ，然后设置 docker0 网桥网段；</li>
<li>如果指定了多个 <code>EnvironmentFile</code> 选项，则必须将 <code>/run/flannel/docker</code> 放在最后(确保 docker0 使用 flanneld 生成的 bip 参数)；</li>
<li>docker 需要以 root 用于运行；</li>
<li>docker 从 1.13 版本开始，可能将 <strong>iptables FORWARD chain的默认策略设置为DROP</strong>，从而导致 ping 其它 Node 上的 Pod IP 失败，遇到这种情况时，需要手动设置策略为 <code>ACCEPT</code>：</li>
</ul>
<pre><code>    $ sudo iptables -P FORWARD ACCEPT

并且把以下命令写入 `/etc/rc.local` 文件中，防止节点重启**iptables FORWARD chain的默认策略又还原为DROP**


    /sbin/iptables -P FORWARD ACCEPT
</code></pre>
<p>完整 unit 见 <a target="_blank" rel="noopener" href="https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/systemd/docker.service">docker.service</a></p>
<p>分发 systemd unit 文件到所有 worker 机器:</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    scp docker.service root@$&#123;node_ip&#125;:/etc/systemd/system/
  done
</code></pre>
<h3 id="配置和分发-docker-配置文件"><a href="#配置和分发-docker-配置文件" class="headerlink" title="配置和分发 docker 配置文件"></a>配置和分发 docker 配置文件</h3><p>使用国内的仓库镜像服务器以加快 pull image 的速度，同时增加下载的并发数 (需要重启 dockerd 生效)：</p>
<pre><code>cat &gt; docker-daemon.json &lt;&lt;EOF
&#123;&quot;insecure-registries&quot;:[&quot;192.168.86.8:5000&quot;,&quot;registry.xxx.com&quot;],
    &quot;registry-mirrors&quot;: [&quot;https://jk4bb75a.mirror.aliyuncs.com&quot;, &quot;https://docker.mirrors.ustc.edu.cn&quot;],
    &quot;max-concurrent-downloads&quot;: 20
&#125;
EOF
</code></pre>
<p>分发 docker 配置文件到所有 work 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    ssh root@$&#123;node_ip&#125; &quot;mkdir -p  /etc/docker/&quot;
    scp docker-daemon.json root@$&#123;node_ip&#125;:/etc/docker/daemon.json
  done
</code></pre>
<h3 id="启动-docker-服务"><a href="#启动-docker-服务" class="headerlink" title="启动 docker 服务"></a>启动 docker 服务</h3><pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    ssh root@$&#123;node_ip&#125; &quot;systemctl stop firewalld &amp;&amp; systemctl disable firewalld&quot;
    ssh root@$&#123;node_ip&#125; &quot;/usr/sbin/iptables -F &amp;&amp; /usr/sbin/iptables -X &amp;&amp; /usr/sbin/iptables -F -t nat &amp;&amp; /usr/sbin/iptables -X -t nat&quot;
    ssh root@$&#123;node_ip&#125; &quot;/usr/sbin/iptables -P FORWARD ACCEPT&quot;
    ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable docker &amp;&amp; systemctl restart docker&quot;
    ssh root@$&#123;node_ip&#125; &#39;for intf in /sys/devices/virtual/net/docker0/brif/*; do echo 1 &gt; $intf/hairpin_mode; done&#39;
    ssh root@$&#123;node_ip&#125; &quot;sudo sysctl -p /etc/sysctl.d/kubernetes.conf&quot;
  done



source /opt/k8s/bin/environment.sh
for node_ip in $&#123;NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
     ssh root@$&#123;node_ip&#125; &quot;systemctl restart docker&quot;
  done
</code></pre>
<ul>
<li>关闭 firewalld(centos7)/ufw(ubuntu16.04)，否则可能会重复创建 iptables 规则；</li>
<li>清理旧的 iptables rules 和 chains 规则；</li>
<li>开启 docker0 网桥下虚拟网卡的 hairpin 模式;</li>
</ul>
<h3 id="检查服务运行状态-1"><a href="#检查服务运行状态-1" class="headerlink" title="检查服务运行状态"></a>检查服务运行状态</h3><pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    ssh k8s@$&#123;node_ip&#125; &quot;systemctl status docker|grep Active&quot;
  done
</code></pre>
<p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p>
<pre><code>$ journalctl -u docker
</code></pre>
<h4 id="检查-docker0-网桥"><a href="#检查-docker0-网桥" class="headerlink" title="检查 docker0 网桥"></a>检查 docker0 网桥</h4><pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    ssh k8s@$&#123;node_ip&#125; &quot;/usr/sbin/ip addr show flannel.1 &amp;&amp; /usr/sbin/ip addr show docker0&quot;
  done
</code></pre>
<p>确认各 work 节点的 docker0 网桥和 flannel.1 接口的 IP 处于同一个网段中(如下 172.30.39.0 和 172.30.39.1)：</p>
<pre><code>3: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default
    link/ether ce:2f:d6:53:e5:f3 brd ff:ff:ff:ff:ff:ff
    inet 172.30.39.0/32 scope global flannel.1
      valid_lft forever preferred_lft forever
    inet6 fe80::cc2f:d6ff:fe53:e5f3/64 scope link
      valid_lft forever preferred_lft forever
4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:bf:65:16:5c brd ff:ff:ff:ff:ff:ff
    inet 172.30.39.1/24 brd 172.30.39.255 scope global docker0
      valid_lft forever preferred_lft forever
</code></pre>
<h2 id="07-2-部署-kubelet-组件"><a href="#07-2-部署-kubelet-组件" class="headerlink" title="07-2.部署 kubelet 组件"></a>07-2.部署 kubelet 组件</h2><p>kublet 运行在每个 worker 节点上，接收 kube-apiserver 发送的请求，管理 Pod 容器，执行交互式命令，如 exec、run、logs 等。</p>
<p>kublet 启动时自动向 kube-apiserver 注册节点信息，内置的 cadvisor 统计和监控节点的资源使用情况。</p>
<p>为确保安全，本文档只开启接收 https 请求的安全端口，对请求进行认证和授权，拒绝未授权的访问(如 apiserver、heapster)。</p>
<h3 id="创建-kubelet-bootstrap-kubeconfig-文件"><a href="#创建-kubelet-bootstrap-kubeconfig-文件" class="headerlink" title="创建 kubelet bootstrap kubeconfig 文件"></a>创建 kubelet bootstrap kubeconfig 文件</h3><pre><code>source /opt/k8s/bin/environment.sh
for node_name in $&#123;NODE_NAMES[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_name&#125;&quot;

    # 创建 token
    export BOOTSTRAP_TOKEN=$(kubeadm token create \
      --description kubelet-bootstrap-token \
      --groups system:bootstrappers:$&#123;node_name&#125; \
      --kubeconfig ~/.kube/config)

    # 设置集群参数
    kubectl config set-cluster kubernetes \
      --certificate-authority=/etc/kubernetes/cert/ca.pem \
      --embed-certs=true \
      --server=$&#123;KUBE_APISERVER&#125; \
      --kubeconfig=kubelet-bootstrap-$&#123;node_name&#125;.kubeconfig

    # 设置客户端认证参数
    kubectl config set-credentials kubelet-bootstrap \
      --token=$&#123;BOOTSTRAP_TOKEN&#125; \
      --kubeconfig=kubelet-bootstrap-$&#123;node_name&#125;.kubeconfig

    # 设置上下文参数
    kubectl config set-context default \
      --cluster=kubernetes \
      --user=kubelet-bootstrap \
      --kubeconfig=kubelet-bootstrap-$&#123;node_name&#125;.kubeconfig

    # 设置默认上下文
    kubectl config use-context default --kubeconfig=kubelet-bootstrap-$&#123;node_name&#125;.kubeconfig
  done
</code></pre>
<ul>
<li>证书中写入 Token 而非证书，证书后续由 controller-manager 创建。</li>
</ul>
<p>查看 kubeadm 为各节点创建的 token：</p>
<pre><code>$ kubeadm token list --kubeconfig ~/.kube/config
TOKEN                     TTL       EXPIRES                     USAGES                   DESCRIPTION               EXTRA GROUPS
k0s2bj.7nvw1zi1nalyz4gz   23h       2018-06-14T15:14:31+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:kube-node1
mkus5s.vilnjk3kutei600l   23h       2018-06-14T15:14:32+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:kube-node3
zkiem5.0m4xhw0jc8r466nk   23h       2018-06-14T15:14:32+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:kube-node2
</code></pre>
<ul>
<li>创建的 token 有效期为 1 天，超期后将不能再被使用，且会被 kube-controller-manager 的 tokencleaner 清理(如果启用该 controller 的话)；</li>
<li>kube-apiserver 接收 kubelet 的 bootstrap token 后，将请求的 user 设置为 system:bootstrap:<token id="">，group 设置为 system:bootstrappers；</token></li>
</ul>
<p>各 token 关联的 Secret：</p>
<pre><code>$ kubectl get secrets  -n kube-system
NAME                     TYPE                                  DATA      AGE
bootstrap-token-k0s2bj   bootstrap.kubernetes.io/token         7         1m
bootstrap-token-mkus5s   bootstrap.kubernetes.io/token         7         1m
bootstrap-token-zkiem5   bootstrap.kubernetes.io/token         7         1m
default-token-99st7      kubernetes.io/service-account-token   3         2d
</code></pre>
<h2 id="分发-bootstrap-kubeconfig-文件到所有-worker-节点"><a href="#分发-bootstrap-kubeconfig-文件到所有-worker-节点" class="headerlink" title="分发 bootstrap kubeconfig 文件到所有 worker 节点"></a>分发 bootstrap kubeconfig 文件到所有 worker 节点</h2><pre><code>source /opt/k8s/bin/environment.sh
for node_name in $&#123;NODE_NAMES[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_name&#125;&quot;
    scp kubelet-bootstrap-$&#123;node_name&#125;.kubeconfig k8s@$&#123;node_name&#125;:/etc/kubernetes/kubelet-bootstrap.kubeconfig
  done
</code></pre>
<h2 id="创建和分发-kubelet-参数配置文件"><a href="#创建和分发-kubelet-参数配置文件" class="headerlink" title="创建和分发 kubelet 参数配置文件"></a>创建和分发 kubelet 参数配置文件</h2><p>从 v1.10 开始，kubelet <strong>部分参数</strong>需在配置文件中配置，<code>kubelet --help</code> 会提示：</p>
<pre><code>DEPRECATED: This parameter should be set via the config file specified by the Kubelet&#39;s --config flag
</code></pre>
<p>创建 kubelet 参数配置模板文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
cat &gt; kubelet.config.json.template &lt;&lt;EOF
&#123;
  &quot;kind&quot;: &quot;KubeletConfiguration&quot;,
  &quot;apiVersion&quot;: &quot;kubelet.config.k8s.io/v1beta1&quot;,
  &quot;authentication&quot;: &#123;
    &quot;x509&quot;: &#123;
      &quot;clientCAFile&quot;: &quot;/etc/kubernetes/cert/ca.pem&quot;
    &#125;,
    &quot;webhook&quot;: &#123;
      &quot;enabled&quot;: true,
      &quot;cacheTTL&quot;: &quot;2m0s&quot;
    &#125;,
    &quot;anonymous&quot;: &#123;
      &quot;enabled&quot;: false
    &#125;
  &#125;,
  &quot;authorization&quot;: &#123;
    &quot;mode&quot;: &quot;Webhook&quot;,
    &quot;webhook&quot;: &#123;
      &quot;cacheAuthorizedTTL&quot;: &quot;5m0s&quot;,
      &quot;cacheUnauthorizedTTL&quot;: &quot;30s&quot;
    &#125;
  &#125;,
  &quot;address&quot;: &quot;##NODE_IP##&quot;,
  &quot;port&quot;: 10250,
  &quot;readOnlyPort&quot;: 0,
  &quot;cgroupDriver&quot;: &quot;cgroupfs&quot;,
  &quot;hairpinMode&quot;: &quot;promiscuous-bridge&quot;,
  &quot;serializeImagePulls&quot;: false,
  &quot;featureGates&quot;: &#123;
    &quot;RotateKubeletClientCertificate&quot;: true,
    &quot;RotateKubeletServerCertificate&quot;: true
  &#125;,
  &quot;clusterDomain&quot;: &quot;$&#123;CLUSTER_DNS_DOMAIN&#125;&quot;,
  &quot;clusterDNS&quot;: [&quot;$&#123;CLUSTER_DNS_SVC_IP&#125;&quot;]
&#125;
EOF
</code></pre>
<ul>
<li>address：API 监听地址，不能为 127.0.0.1，否则 kube-apiserver、heapster 等不能调用 kubelet 的 API；</li>
<li>readOnlyPort=0：关闭只读端口(默认 10255)，等效为未指定；</li>
<li>authentication.anonymous.enabled：设置为 false，不允许匿名访问 10250 端口；</li>
<li>authentication.x509.clientCAFile：指定签名客户端证书的 CA 证书，开启 HTTP 证书认证；</li>
<li>authentication.webhook.enabled=true：开启 HTTPs bearer token 认证；</li>
<li>对于未通过 x509 证书和 webhook 认证的请求(kube-apiserver 或其他客户端)，将被拒绝，提示 Unauthorized；</li>
<li>authroization.mode=Webhook：kubelet 使用 SubjectAccessReview API 查询 kube-apiserver 某 user、group 是否具有操作资源的权限(RBAC)；</li>
<li>featureGates.RotateKubeletClientCertificate、featureGates.RotateKubeletServerCertificate：自动 rotate 证书，证书的有效期取决于 kube-controller-manager 的 –experimental-cluster-signing-duration 参数；</li>
<li>需要 root 账户运行；</li>
</ul>
<p>为各节点创建和分发 kubelet 配置文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;NODE_IPS[@]&#125;
  do 
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    sed -e &quot;s/##NODE_IP##/$&#123;node_ip&#125;/&quot; kubelet.config.json.template &gt; kubelet.config-$&#123;node_ip&#125;.json
    scp kubelet.config-$&#123;node_ip&#125;.json root@$&#123;node_ip&#125;:/etc/kubernetes/kubelet.config.json
  done
</code></pre>
<p>替换后的 kubelet.config.json 文件： <a target="_blank" rel="noopener" href="https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/systemd/kubelet.config.json">kubelet.config.json</a></p>
<h2 id="创建和分发-kubelet-systemd-unit-文件"><a href="#创建和分发-kubelet-systemd-unit-文件" class="headerlink" title="创建和分发 kubelet systemd unit 文件"></a>创建和分发 kubelet systemd unit 文件</h2><p>创建 kubelet systemd unit 文件模板：</p>
<pre><code>cat &gt; kubelet.service.template &lt;&lt;EOF
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
ExecStart=/opt/k8s/bin/kubelet \\
  --bootstrap-kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig \\
  --cert-dir=/etc/kubernetes/cert \\
  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\
  --config=/etc/kubernetes/kubelet.config.json \\
  --hostname-override=##NODE_NAME## \\
  --pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest \\
  --allow-privileged=true \\
  --alsologtostderr=true \\
  --logtostderr=false \\
  --log-dir=/var/log/kubernetes \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<ul>
<li>如果设置了 <code>--hostname-override</code> 选项，则 <code>kube-proxy</code> 也需要设置该选项，否则会出现找不到 Node 的情况；</li>
<li><code>--bootstrap-kubeconfig</code>：指向 bootstrap kubeconfig 文件，kubelet 使用该文件中的用户名和 token 向 kube-apiserver 发送 TLS Bootstrapping 请求；</li>
<li>K8S approve kubelet 的 csr 请求后，在 <code>--cert-dir</code> 目录创建证书和私钥文件，然后写入 <code>--kubeconfig</code> 文件；</li>
</ul>
<p>替换后的 unit 文件：<a target="_blank" rel="noopener" href="https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/systemd/kubelet.service">kubelet.service</a></p>
<p>为各节点创建和分发 kubelet systemd unit 文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_name in $&#123;NODE_NAMES[@]&#125;
  do 
    echo &quot;&gt;&gt;&gt; $&#123;node_name&#125;&quot;
    sed -e &quot;s/##NODE_NAME##/$&#123;node_name&#125;/&quot; kubelet.service.template &gt; kubelet-$&#123;node_name&#125;.service
    scp kubelet-$&#123;node_name&#125;.service root@$&#123;node_name&#125;:/etc/systemd/system/kubelet.service
  done
</code></pre>
<h2 id="Bootstrap-Token-Auth-和授予权限"><a href="#Bootstrap-Token-Auth-和授予权限" class="headerlink" title="Bootstrap Token Auth 和授予权限"></a>Bootstrap Token Auth 和授予权限</h2><p>kublet 启动时查找配置的 –kubeletconfig 文件是否存在，如果不存在则使用 –bootstrap-kubeconfig 向 kube-apiserver 发送证书签名请求 (CSR)。</p>
<p>kube-apiserver 收到 CSR 请求后，对其中的 Token 进行认证（事先使用 kubeadm 创建的 token），认证通过后将请求的 user 设置为 system:bootstrap:<token id="">，group 设置为 system:bootstrappers，这一过程称为 Bootstrap Token Auth。</token></p>
<p>默认情况下，这个 user 和 group 没有创建 CSR 的权限:q，kubelet 启动失败，错误日志如下：</p>
<pre><code>$ sudo journalctl -u kubelet -a |grep -A 2 &#39;certificatesigningrequests&#39;
May 06 06:42:36 kube-node1 kubelet[26986]: F0506 06:42:36.314378   26986 server.go:233] failed to run Kubelet: cannot create certificate signing request: certificatesigningrequests.certificates.k8s.io is forbidden: User &quot;system:bootstrap:lemy40&quot; cannot create certificatesigningrequests.certificates.k8s.io at the cluster scope
May 06 06:42:36 kube-node1 systemd[1]: kubelet.service: Main process exited, code=exited, status=255/n/a
May 06 06:42:36 kube-node1 systemd[1]: kubelet.service: Failed with result &#39;exit-code&#39;.
</code></pre>
<p>解决办法是：创建一个 clusterrolebinding，将 group system:bootstrappers 和 clusterrole system:node-bootstrapper 绑定：</p>
<pre><code>$ kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers
</code></pre>
<h2 id="启动-kubelet-服务"><a href="#启动-kubelet-服务" class="headerlink" title="启动 kubelet 服务"></a>启动 kubelet 服务</h2><pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;ETCD_NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    ssh root@$&#123;node_ip&#125; &quot;mkdir -p /var/lib/kubelet&quot;
    ssh root@$&#123;node_ip&#125; &quot;/usr/sbin/swapoff -a&quot;
    ssh root@$&#123;node_ip&#125; &quot;mkdir -p /var/log/kubernetes &amp;&amp; chown -R k8s /var/log/kubernetes&quot;
    ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable kubelet &amp;&amp; systemctl restart kubelet&quot;
  done
</code></pre>
<ul>
<li>关闭 swap 分区，否则 kubelet 会启动失败；</li>
<li>必须先创建工作和日志目录；</li>
</ul>
<p>source /opt/k8s/bin/environment.sh<br> for node_ip in ${NODE_IPS[@]}<br> do<br> echo “&gt;&gt;&gt; ${node_ip}”<br> ssh root@${node_ip} “systemctl restart kubelet &amp;&amp; systemctl status kubelet|grep Active:”<br> done</p>
<pre><code>$ journalctl -u kubelet |tail
Jun 13 16:05:40 kube-node2 kubelet[22343]: I0613 16:05:40.388242   22343 feature_gate.go:226] feature gates: &amp;&#123;&#123;&#125; map[RotateKubeletServerCertificate:true RotateKubeletClientCertificate:true]&#125;
Jun 13 16:05:40 kube-node2 kubelet[22343]: I0613 16:05:40.394342   22343 mount_linux.go:211] Detected OS with systemd
Jun 13 16:05:40 kube-node2 kubelet[22343]: W0613 16:05:40.394494   22343 cni.go:171] Unable to update cni config: No networks found in /etc/cni/net.d
Jun 13 16:05:40 kube-node2 kubelet[22343]: I0613 16:05:40.399508   22343 server.go:376] Version: v1.10.4
Jun 13 16:05:40 kube-node2 kubelet[22343]: I0613 16:05:40.399583   22343 feature_gate.go:226] feature gates: &amp;&#123;&#123;&#125; map[RotateKubeletServerCertificate:true RotateKubeletClientCertificate:true]&#125;
Jun 13 16:05:40 kube-node2 kubelet[22343]: I0613 16:05:40.399736   22343 plugins.go:89] No cloud provider specified.
Jun 13 16:05:40 kube-node2 kubelet[22343]: I0613 16:05:40.399752   22343 server.go:492] No cloud provider specified: &quot;&quot; from the config file: &quot;&quot;
Jun 13 16:05:40 kube-node2 kubelet[22343]: I0613 16:05:40.399777   22343 bootstrap.go:58] Using bootstrap kubeconfig to generate TLS client cert, key and kubeconfig file
Jun 13 16:05:40 kube-node2 kubelet[22343]: I0613 16:05:40.446068   22343 csr.go:105] csr for this node already exists, reusing
Jun 13 16:05:40 kube-node2 kubelet[22343]: I0613 16:05:40.453761   22343 csr.go:113] csr for this node is still valid
</code></pre>
<p>kubelet 启动后使用 –bootstrap-kubeconfig 向 kube-apiserver 发送 CSR 请求，当这个 CSR 被 approve 后，kube-controller-manager 为 kubelet 创建 TLS 客户端证书、私钥和 –kubeletconfig 文件。</p>
<p>注意：kube-controller-manager 需要配置 <code>--cluster-signing-cert-file</code> 和 <code>--cluster-signing-key-file</code> 参数，才会为 TLS Bootstrap 创建证书和私钥。</p>
<pre><code>$ kubectl get csr
NAME                                                   AGE       REQUESTOR                 CONDITION
node-csr-QzuuQiuUfcSdp3j5W4B2UOuvQ_n9aTNHAlrLzVFiqrk   43s       system:bootstrap:zkiem5   Pending
node-csr-oVbPmU-ikVknpynwu0Ckz_MvkAO_F1j0hmbcDa__sGA   27s       system:bootstrap:mkus5s   Pending
node-csr-u0E1-ugxgotO_9FiGXo8DkD6a7-ew8sX2qPE6KPS2IY   13m       system:bootstrap:k0s2bj   Pending

$ kubectl get nodes
No resources found.
</code></pre>
<ul>
<li>三个 work 节点的 csr 均处于 pending 状态；</li>
</ul>
<h2 id="approve-kubelet-CSR-请求"><a href="#approve-kubelet-CSR-请求" class="headerlink" title="approve kubelet CSR 请求"></a>approve kubelet CSR 请求</h2><p>可以手动或自动 approve CSR 请求。推荐使用自动的方式，因为从 v1.8 版本开始，可以自动轮转approve csr 后生成的证书。</p>
<h3 id="手动-approve-CSR-请求"><a href="#手动-approve-CSR-请求" class="headerlink" title="手动 approve CSR 请求"></a>手动 approve CSR 请求</h3><p>查看 CSR 列表：</p>
<pre><code>$ kubectl get csr
NAME                                                   AGE       REQUESTOR                 CONDITION
node-csr-QzuuQiuUfcSdp3j5W4B2UOuvQ_n9aTNHAlrLzVFiqrk   43s       system:bootstrap:zkiem5   Pending
node-csr-oVbPmU-ikVknpynwu0Ckz_MvkAO_F1j0hmbcDa__sGA   27s       system:bootstrap:mkus5s   Pending
node-csr-u0E1-ugxgotO_9FiGXo8DkD6a7-ew8sX2qPE6KPS2IY   13m       system:bootstrap:k0s2bj   Pending
</code></pre>
<p>approve CSR：</p>
<pre><code>$ kubectl certificate approve node-csr-QzuuQiuUfcSdp3j5W4B2UOuvQ_n9aTNHAlrLzVFiqrk
certificatesigningrequest.certificates.k8s.io &quot;node-csr-QzuuQiuUfcSdp3j5W4B2UOuvQ_n9aTNHAlrLzVFiqrk&quot; approved
</code></pre>
<p>查看 Approve 结果：</p>
<pre><code>$ kubectl describe  csr node-csr-QzuuQiuUfcSdp3j5W4B2UOuvQ_n9aTNHAlrLzVFiqrk
Name:               node-csr-QzuuQiuUfcSdp3j5W4B2UOuvQ_n9aTNHAlrLzVFiqrk
Labels:             &lt;none&gt;
Annotations:        &lt;none&gt;
CreationTimestamp:  Wed, 13 Jun 2018 16:05:04 +0800
Requesting User:    system:bootstrap:zkiem5
Status:             Approved
Subject:
         Common Name:    system:node:kube-node2
         Serial Number:
         Organization:   system:nodes
Events:  &lt;none&gt;
</code></pre>
<ul>
<li><code>Requesting User</code>：请求 CSR 的用户，kube-apiserver 对它进行认证和授权；</li>
<li><code>Subject</code>：请求签名的证书信息；</li>
<li>证书的 CN 是 system:node:kube-node2， Organization 是 system:nodes，kube-apiserver 的 Node 授权模式会授予该证书的相关权限；</li>
</ul>
<h3 id="自动-approve-CSR-请求"><a href="#自动-approve-CSR-请求" class="headerlink" title="自动 approve CSR 请求"></a>自动 approve CSR 请求</h3><p>创建三个 ClusterRoleBinding，分别用于自动 approve client、renew client、renew server 证书：</p>
<pre><code>cat &gt; csr-crb.yaml &lt;&lt;EOF
 # Approve all CSRs for the group &quot;system:bootstrappers&quot;
 kind: ClusterRoleBinding
 apiVersion: rbac.authorization.k8s.io/v1
 metadata:
   name: auto-approve-csrs-for-group
 subjects:
 - kind: Group
   name: system:bootstrappers
   apiGroup: rbac.authorization.k8s.io
 roleRef:
   kind: ClusterRole
   name: system:certificates.k8s.io:certificatesigningrequests:nodeclient
   apiGroup: rbac.authorization.k8s.io
---
 # To let a node of the group &quot;system:nodes&quot; renew its own credentials
 kind: ClusterRoleBinding
 apiVersion: rbac.authorization.k8s.io/v1
 metadata:
   name: node-client-cert-renewal
 subjects:
 - kind: Group
   name: system:nodes
   apiGroup: rbac.authorization.k8s.io
 roleRef:
   kind: ClusterRole
   name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient
   apiGroup: rbac.authorization.k8s.io
---
# A ClusterRole which instructs the CSR approver to approve a node requesting a
# serving cert matching its client cert.
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: approve-node-server-renewal-csr
rules:
- apiGroups: [&quot;certificates.k8s.io&quot;]
  resources: [&quot;certificatesigningrequests/selfnodeserver&quot;]
  verbs: [&quot;create&quot;]
---
 # To let a node of the group &quot;system:nodes&quot; renew its own server credentials
 kind: ClusterRoleBinding
 apiVersion: rbac.authorization.k8s.io/v1
 metadata:
   name: node-server-cert-renewal
 subjects:
 - kind: Group
   name: system:nodes
   apiGroup: rbac.authorization.k8s.io
 roleRef:
   kind: ClusterRole
   name: approve-node-server-renewal-csr
   apiGroup: rbac.authorization.k8s.io
EOF
</code></pre>
<ul>
<li>auto-approve-csrs-for-group：自动 approve node 的第一次 CSR； 注意第一次 CSR 时，请求的 Group 为 system:bootstrappers；</li>
<li>node-client-cert-renewal：自动 approve node 后续过期的 client 证书，自动生成的证书 Group 为 system:nodes;</li>
<li>node-server-cert-renewal：自动 approve node 后续过期的 server 证书，自动生成的证书 Group 为 system:nodes;</li>
</ul>
<p>生效配置：</p>
<pre><code>$ kubectl apply -f csr-crb.yaml
</code></pre>
<h2 id="查看-kublet-的情况"><a href="#查看-kublet-的情况" class="headerlink" title="查看 kublet 的情况"></a>查看 kublet 的情况</h2><p>等待一段时间(1-10 分钟)，三个节点的 CSR 都被自动 approve：</p>
<pre><code>$ kubectl get csr
NAME                                                   AGE       REQUESTOR                 CONDITION
csr-98h25                                              6m        system:node:kube-node2    Approved,Issued
csr-lb5c9                                              7m        system:node:kube-node3    Approved,Issued
csr-m2hn4                                              14m       system:node:kube-node1    Approved,Issued平时
node-csr-7q7i0q4MF_K2TSEJj16At4CJFLlJkHIqei6nMIAaJCU   28m       system:bootstrap:k0s2bj   Approved,Issued
node-csr-ND77wk2P8k2lHBtgBaObiyYw0uz1Um7g2pRvveMF-c4   35m       system:bootstrap:mkus5s   Approved,Issued
node-csr-Nysmrw55nnM48NKwEJuiuCGmZoxouK4N8jiEHBtLQso   6m        system:bootstrap:zkiem5   Approved,Issued
node-csr-QzuuQiuUfcSdp3j5W4B2UOuvQ_n9aTNHAlrLzVFiqrk   1h        system:bootstrap:zkiem5   Approved,Issued
node-csr-oVbPmU-ikVknpynwu0Ckz_MvkAO_F1j0hmbcDa__sGA   1h        system:bootstrap:mkus5s   Approved,Issued
node-csr-u0E1-ugxgotO_9FiGXo8DkD6a7-ew8sX2qPE6KPS2IY   1h        system:bootstrap:k0s2bj   Approved,Issued
</code></pre>
<p>所有节点均 ready：</p>
<pre><code>$ kubectl get nodes
NAME         STATUS    ROLES     AGE       VERSION
kube-node1   Ready     &lt;none&gt;    18m       v1.10.4
kube-node2   Ready     &lt;none&gt;    10m       v1.10.4
kube-node3   Ready     &lt;none&gt;    11m       v1.10.4
</code></pre>
<p>kube-controller-manager 为各 node 生成了 kubeconfig 文件和公私钥：</p>
<pre><code>$ ls -l /etc/kubernetes/kubelet.kubeconfig
-rw------- 1 root root 2293 Jun 13 17:07 /etc/kubernetes/kubelet.kubeconfig

$ ls -l /etc/kubernetes/cert/|grep kubelet
-rw-r--r-- 1 root root 1046 Jun 13 17:07 kubelet-client.crt
-rw------- 1 root root  227 Jun 13 17:07 kubelet-client.key
-rw------- 1 root root 1334 Jun 13 17:07 kubelet-server-2018-06-13-17-07-45.pem
lrwxrwxrwx 1 root root   58 Jun 13 17:07 kubelet-server-current.pem -&gt; /etc/kubernetes/cert/kubelet-server-2018-06-13-17-07-45.pem
</code></pre>
<ul>
<li>kubelet-server 证书会周期轮转；</li>
</ul>
<h2 id="kubelet-提供的-API-接口"><a href="#kubelet-提供的-API-接口" class="headerlink" title="kubelet 提供的 API 接口"></a>kubelet 提供的 API 接口</h2><p>kublet 启动后监听多个端口，用于接收 kube-apiserver 或其它组件发送的请求：</p>
<pre><code>$ sudo netstat -lnpt|grep kubelet
tcp        0      0 172.27.129.111:4194     0.0.0.0:*               LISTEN      2490/kubelet
tcp        0      0 127.0.0.1:10248         0.0.0.0:*               LISTEN      2490/kubelet
tcp        0      0 172.27.129.111:10250    0.0.0.0:*               LISTEN      2490/kubelet
</code></pre>
<ul>
<li>4194: cadvisor http 服务；</li>
<li>10248: healthz http 服务；</li>
<li>10250: https API 服务；注意：未开启只读端口 10255；</li>
</ul>
<p>例如执行 <code>kubectl ec -it nginx-ds-5rmws -- sh</code> 命令时，kube-apiserver 会向 kubelet 发送如下请求：</p>
<pre><code>POST /exec/default/nginx-ds-5rmws/my-nginx?command=sh&amp;input=1&amp;output=1&amp;tty=1
</code></pre>
<p>kubelet 接收 10250 端口的 https 请求：</p>
<ul>
<li>/pods、/runningpods</li>
<li>/metrics、/metrics/cadvisor、/metrics/probes</li>
<li>/spec</li>
<li>/stats、/stats/container</li>
<li>/logs</li>
<li>/run/、”/exec/“, “/attach/“, “/portForward/“, “/containerLogs/“ 等管理；</li>
</ul>
<p>详情参考：<a target="_blank" rel="noopener" href="https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/server/server.go#L434:3">https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/server/server.go#L434:3</a></p>
<p>由于关闭了匿名认证，同时开启了 webhook 授权，所有访问 10250 端口 https API 的请求都需要被认证和授权。</p>
<p>预定义的 ClusterRole system:kubelet-api-admin 授予访问 kubelet 所有 API 的权限：</p>
<pre><code>$ kubectl describe clusterrole system:kubelet-api-admin
Name:         system:kubelet-api-admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources      Non-Resource URLs  Resource Names  Verbs
  ---------      -----------------  --------------  -----
  nodes          []                 []              [get list watch proxy]
  nodes/log      []                 []              [*]
  nodes/metrics  []                 []              [*]
  nodes/proxy    []                 []              [*]
  nodes/spec     []                 []              [*]
  nodes/stats    []                 []              [*]
</code></pre>
<h2 id="kublet-api-认证和授权"><a href="#kublet-api-认证和授权" class="headerlink" title="kublet api 认证和授权"></a>kublet api 认证和授权</h2><p>kublet 配置了如下认证参数：</p>
<ul>
<li>authentication.anonymous.enabled：设置为 false，不允许匿名访问 10250 端口；</li>
<li>authentication.x509.clientCAFile：指定签名客户端证书的 CA 证书，开启 HTTPs 证书认证；</li>
<li>authentication.webhook.enabled=true：开启 HTTPs bearer token 认证；</li>
</ul>
<p>同时配置了如下授权参数：</p>
<ul>
<li>authroization.mode=Webhook：开启 RBAC 授权；</li>
</ul>
<p>kubelet 收到请求后，使用 clientCAFile 对证书签名进行认证，或者查询 bearer token 是否有效。如果两者都没通过，则拒绝请求，提示 Unauthorized：</p>
<pre><code>$ curl -s --cacert /etc/kubernetes/cert/ca.pem https://192.168.86.156:10250/metrics
Unauthorized

$ curl -s --cacert /etc/kubernetes/cert/ca.pem -H &quot;Authorization: Bearer 123456&quot; https://172.27.129.111:10250/metrics
Unauthorized
</code></pre>
<p>通过认证后，kubelet 使用 SubjectAccessReview API 向 kube-apiserver 发送请求，查询证书或 token 对应的 user、group 是否有操作资源的权限(RBAC)；</p>
<p>证书认证和授权：</p>
<pre><code>$ # 权限不足的证书；
$ curl -s --cacert /etc/kubernetes/cert/ca.pem --cert /etc/kubernetes/cert/kube-controller-manager.pem --key /etc/kubernetes/cert/kube-controller-manager-key.pem https://172.27.129.111:10250/metrics
Forbidden (user=system:kube-controller-manager, verb=get, resource=nodes, subresource=metrics)

$ # 使用部署 kubectl 命令行工具时创建的、具有最高权限的 admin 证书；
$ curl -s --cacert /etc/kubernetes/cert/ca.pem --cert ./admin.pem --key ./admin-key.pem https://192.168.86.156:10250/metrics|head
# HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.
# TYPE apiserver_client_certificate_expiration_seconds histogram
apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;0&quot;&#125; 0
apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;21600&quot;&#125; 0
apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;43200&quot;&#125; 0
apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;86400&quot;&#125; 0
apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;172800&quot;&#125; 0
apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;345600&quot;&#125; 0
apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;604800&quot;&#125; 0
apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;2.592e+06&quot;&#125; 0
</code></pre>
<ul>
<li><code>--cacert</code>、<code>--cert</code>、<code>--key</code> 的参数值必须是文件路径，如上面的 <code>./admin.pem</code> 不能省略 <code>./</code>，否则返回 <code>401 Unauthorized</code>；</li>
</ul>
<p>bear token 认证和授权：</p>
<p>创建一个 ServiceAccount，将它和 ClusterRole system:kubelet-api-admin 绑定，从而具有调用 kubelet API 的权限：</p>
<pre><code>kubectl create sa kubelet-api-test
kubectl create clusterrolebinding kubelet-api-test --clusterrole=system:kubelet-api-admin --serviceaccount=default:kubelet-api-test
SECRET=$(kubectl get secrets | grep kubelet-api-test | awk &#39;&#123;print $1&#125;&#39;)
TOKEN=$(kubectl describe secret $&#123;SECRET&#125; | grep -E &#39;^token&#39; | awk &#39;&#123;print $2&#125;&#39;)
echo $&#123;TOKEN&#125;

$ curl -s --cacert /etc/kubernetes/cert/ca.pem -H &quot;Authorization: Bearer $&#123;TOKEN&#125;&quot; https://172.27.129.111:10250/metrics|head
# HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.
# TYPE apiserver_client_certificate_expiration_seconds histogram
apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;0&quot;&#125; 0
apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;21600&quot;&#125; 0
apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;43200&quot;&#125; 0
apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;86400&quot;&#125; 0
apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;172800&quot;&#125; 0
apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;345600&quot;&#125; 0
apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;604800&quot;&#125; 0
apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;2.592e+06&quot;&#125; 0
</code></pre>
<h3 id="cadvisor-和-metrics"><a href="#cadvisor-和-metrics" class="headerlink" title="cadvisor 和 metrics"></a>cadvisor 和 metrics</h3><p>cadvisor 统计所在节点各容器的资源(CPU、内存、磁盘、网卡)使用情况，分别在自己的 http web 页面(4194 端口)和 10250 以 promehteus metrics 的形式输出。</p>
<p>浏览器访问 <a target="_blank" rel="noopener" href="http://172.27.129.105:4194/containers/">http://172.27.129.105:4194/containers/</a> 可以查看到 cadvisor 的监控页面：</p>
<p><img src="/jadepeng/images/cadvisor-home.png" alt="cadvisor-home"></p>
<p>浏览器访问 <a target="_blank" rel="noopener" href="https://172.27.129.80:10250/metrics">https://172.27.129.80:10250/metrics</a> 和 <a target="_blank" rel="noopener" href="https://172.27.129.80:10250/metrics/cadvisor">https://172.27.129.80:10250/metrics/cadvisor</a> 分别返回 kublet 和 cadvisor 的 metrics。</p>
<p><img src="/jadepeng/images/cadvisor-metrics.png" alt="cadvisor-metrics"></p>
<p>注意：</p>
<ul>
<li>kublet.config.json 设置 authentication.anonymous.enabled 为 false，不允许匿名证书访问 10250 的 https 服务；</li>
<li>参考<a href="A.%E6%B5%8F%E8%A7%88%E5%99%A8%E8%AE%BF%E9%97%AEkube-apiserver%E5%AE%89%E5%85%A8%E7%AB%AF%E5%8F%A3.md">A.浏览器访问kube-apiserver安全端口.md</a>，创建和导入相关证书，然后访问上面的 10250 端口；</li>
</ul>
<h2 id="获取-kublet-的配置"><a href="#获取-kublet-的配置" class="headerlink" title="获取 kublet 的配置"></a>获取 kublet 的配置</h2><p>从 kube-apiserver 获取各 node 的配置：</p>
<p>curl -sSL –cacert /etc/kubernetes/cert/ca.pem –cert ./admin.pem –key ./admin-key.pem <a target="_blank" rel="noopener" href="https://192.168.86.214:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy">https://192.168.86.214:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</a></p>
<pre><code>$ source /opt/k8s/bin/environment.sh
$ # 使用部署 kubectl 命令行工具时创建的、具有最高权限的 admin 证书；
$ curl -sSL --cacert /etc/kubernetes/cert/ca.pem --cert ./admin.pem --key ./admin-key.pem $&#123;KUBE_APISERVER&#125;/api/v1/nodes/docker86-155/proxy/configz | jq \
  &#39;.kubeletconfig|.kind=&quot;KubeletConfiguration&quot;|.apiVersion=&quot;kubelet.config.k8s.io/v1beta1&quot;&#39;
&#123;
  &quot;syncFrequency&quot;: &quot;1m0s&quot;,
  &quot;fileCheckFrequency&quot;: &quot;20s&quot;,
  &quot;httpCheckFrequency&quot;: &quot;20s&quot;,
  &quot;address&quot;: &quot;172.27.129.80&quot;,
  &quot;port&quot;: 10250,
  &quot;readOnlyPort&quot;: 10255,
  &quot;authentication&quot;: &#123;
    &quot;x509&quot;: &#123;&#125;,
    &quot;webhook&quot;: &#123;
      &quot;enabled&quot;: false,
      &quot;cacheTTL&quot;: &quot;2m0s&quot;
    &#125;,
    &quot;anonymous&quot;: &#123;
      &quot;enabled&quot;: true
    &#125;
  &#125;,
  &quot;authorization&quot;: &#123;
    &quot;mode&quot;: &quot;AlwaysAllow&quot;,
    &quot;webhook&quot;: &#123;
      &quot;cacheAuthorizedTTL&quot;: &quot;5m0s&quot;,
      &quot;cacheUnauthorizedTTL&quot;: &quot;30s&quot;
    &#125;
  &#125;,
  &quot;registryPullQPS&quot;: 5,
  &quot;registryBurst&quot;: 10,
  &quot;eventRecordQPS&quot;: 5,
  &quot;eventBurst&quot;: 10,
  &quot;enableDebuggingHandlers&quot;: true,
  &quot;healthzPort&quot;: 10248,
  &quot;healthzBindAddress&quot;: &quot;127.0.0.1&quot;,
  &quot;oomScoreAdj&quot;: -999,
  &quot;clusterDomain&quot;: &quot;cluster.local.&quot;,
  &quot;clusterDNS&quot;: [
    &quot;10.254.0.2&quot;
  ],
  &quot;streamingConnectionIdleTimeout&quot;: &quot;4h0m0s&quot;,
  &quot;nodeStatusUpdateFrequency&quot;: &quot;10s&quot;,
  &quot;imageMinimumGCAge&quot;: &quot;2m0s&quot;,
  &quot;imageGCHighThresholdPercent&quot;: 85,
  &quot;imageGCLowThresholdPercent&quot;: 80,
  &quot;volumeStatsAggPeriod&quot;: &quot;1m0s&quot;,
  &quot;cgroupsPerQOS&quot;: true,
  &quot;cgroupDriver&quot;: &quot;cgroupfs&quot;,
  &quot;cpuManagerPolicy&quot;: &quot;none&quot;,
  &quot;cpuManagerReconcilePeriod&quot;: &quot;10s&quot;,
  &quot;runtimeRequestTimeout&quot;: &quot;2m0s&quot;,
  &quot;hairpinMode&quot;: &quot;promiscuous-bridge&quot;,
  &quot;maxPods&quot;: 110,
  &quot;podPidsLimit&quot;: -1,
  &quot;resolvConf&quot;: &quot;/etc/resolv.conf&quot;,
  &quot;cpuCFSQuota&quot;: true,
  &quot;maxOpenFiles&quot;: 1000000,
  &quot;contentType&quot;: &quot;application/vnd.kubernetes.protobuf&quot;,
  &quot;kubeAPIQPS&quot;: 5,
  &quot;kubeAPIBurst&quot;: 10,
  &quot;serializeImagePulls&quot;: false,
  &quot;evictionHard&quot;: &#123;
    &quot;imagefs.available&quot;: &quot;15%&quot;,
    &quot;memory.available&quot;: &quot;100Mi&quot;,
    &quot;nodefs.available&quot;: &quot;10%&quot;,
    &quot;nodefs.inodesFree&quot;: &quot;5%&quot;
  &#125;,
  &quot;evictionPressureTransitionPeriod&quot;: &quot;5m0s&quot;,
  &quot;enableControllerAttachDetach&quot;: true,
  &quot;makeIPTablesUtilChains&quot;: true,
  &quot;iptablesMasqueradeBit&quot;: 14,
  &quot;iptablesDropBit&quot;: 15,
  &quot;featureGates&quot;: &#123;
    &quot;RotateKubeletClientCertificate&quot;: true,
    &quot;RotateKubeletServerCertificate&quot;: true
  &#125;,
  &quot;failSwapOn&quot;: true,
  &quot;containerLogMaxSize&quot;: &quot;10Mi&quot;,
  &quot;containerLogMaxFiles&quot;: 5,
  &quot;enforceNodeAllocatable&quot;: [
    &quot;pods&quot;
  ],
  &quot;kind&quot;: &quot;KubeletConfiguration&quot;,
  &quot;apiVersion&quot;: &quot;kubelet.config.k8s.io/v1beta1&quot;
&#125;
</code></pre>
<p>或者参考代码中的注释：<a target="_blank" rel="noopener" href="https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/kubeletconfig/v1beta1/types.go">https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/kubeletconfig/v1beta1/types.go</a></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li>kubelet 认证和授权：<a target="_blank" rel="noopener" href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/">https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/</a></li>
</ol>
<p>source /opt/k8s/bin/environment.sh<br> for node_ip in ${NODE_IPS[@]}<br> do<br> echo “&gt;&gt;&gt; ${node_ip}”<br> ssh root@${node_ip} “systemctl daemon-reload &amp;&amp; systemctl enable kubelet &amp;&amp; systemctl restart kubelet”<br> done</p>
<p>source /opt/k8s/bin/environment.sh</p>
<p>for node_ip in ${ETCD_NODE_IPS[@]}<br> do<br> echo “&gt;&gt;&gt; ${node_ip}”<br> ssh root@${node_ip} “mkdir -p /var/lib/kube-proxy”<br> ssh root@${node_ip} “mkdir -p /var/log/kubernetes &amp;&amp; chown -R k8s /var/log/kubernetes”<br> ssh root@${node_ip} “systemctl daemon-reload &amp;&amp; systemctl enable kube-proxy &amp;&amp; systemctl restart kube-proxy”<br> done</p>
<p>source /opt/k8s/bin/environment.sh</p>
<p>for node_ip in ${NODE_IPS[@]}<br> do<br> echo “&gt;&gt;&gt; ${node_ip}”<br> scp /usr/local/bin/pull-google-container root@${node_ip}:/usr/local/bin/<br> ssh root@${node_ip} “/usr/local/bin/pull-google-container k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.0”<br> done</p>
<p>192.168.86.18 192.168.86.21 192.168.86.91 192.168.86.9</p>
<p>cat &lt;&lt;EOF | kubectl apply -f -<br> kind: ClusterRoleBinding<br> apiVersion: rbac.authorization.k8s.io/v1beta1<br> metadata:<br> name: heapster-kubelet-api<br> roleRef:<br> apiGroup: rbac.authorization.k8s.io<br> kind: ClusterRole<br> name: system:kubelet-api-admin<br> subjects:</p>
<ul>
<li>kind: ServiceAccount  </li>
</ul>
<p>name: heapster  </p>
<p>namespace: kube-system  </p>
<p>EOF</p>
<h2 id="07-3-部署-kube-proxy-组件"><a href="#07-3-部署-kube-proxy-组件" class="headerlink" title="07-3.部署 kube-proxy 组件"></a>07-3.部署 kube-proxy 组件</h2><p>kube-proxy 运行在所有 worker 节点上，，它监听 apiserver 中 service 和 Endpoint 的变化情况，创建路由规则来进行服务负载均衡。</p>
<p>本文档讲解部署 kube-proxy 的部署，使用 ipvs 模式。</p>
<h3 id="下载和分发-kube-proxy-二进制文件"><a href="#下载和分发-kube-proxy-二进制文件" class="headerlink" title="下载和分发 kube-proxy 二进制文件"></a>下载和分发 kube-proxy 二进制文件</h3><p>参考 <a href="06-0.%E9%83%A8%E7%BD%B2master%E8%8A%82%E7%82%B9.md">06-0.部署master节点.md</a></p>
<h3 id="安装依赖包-1"><a href="#安装依赖包-1" class="headerlink" title="安装依赖包"></a>安装依赖包</h3><p>各节点需要安装 <code>ipvsadm</code> 和 <code>ipset</code> 命令，加载 <code>ip_vs</code> 内核模块。</p>
<p>参考 <a href="07-0.%E9%83%A8%E7%BD%B2worker%E8%8A%82%E7%82%B9.md">07-0.部署worker节点.md</a></p>
<h3 id="创建-kube-proxy-证书"><a href="#创建-kube-proxy-证书" class="headerlink" title="创建 kube-proxy 证书"></a>创建 kube-proxy 证书</h3><p>创建证书签名请求：</p>
<pre><code>cat &gt; kube-proxy-csr.json &lt;&lt;EOF
&#123;
  &quot;CN&quot;: &quot;system:kube-proxy&quot;,
  &quot;key&quot;: &#123;
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  &#125;,
  &quot;names&quot;: [
    &#123;
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;4Paradigm&quot;
    &#125;
  ]
&#125;
EOF
</code></pre>
<ul>
<li>CN：指定该证书的 User 为 <code>system:kube-proxy</code>；</li>
<li>预定义的 RoleBinding <code>system:node-proxier</code> 将User <code>system:kube-proxy</code> 与 Role <code>system:node-proxier</code> 绑定，该 Role 授予了调用 <code>kube-apiserver</code> Proxy 相关 API 的权限；</li>
<li>该证书只会被 kube-proxy 当做 client 证书使用，所以 hosts 字段为空；</li>
</ul>
<p>生成证书和私钥：</p>
<pre><code>cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
  -ca-key=/etc/kubernetes/cert/ca-key.pem \
  -config=/etc/kubernetes/cert/ca-config.json \
  -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy
</code></pre>
<h2 id="创建和分发-kubeconfig-文件-2"><a href="#创建和分发-kubeconfig-文件-2" class="headerlink" title="创建和分发 kubeconfig 文件"></a>创建和分发 kubeconfig 文件</h2><pre><code>source /opt/k8s/bin/environment.sh
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/cert/ca.pem \
  --embed-certs=true \
  --server=$&#123;KUBE_APISERVER&#125; \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config set-credentials kube-proxy \
  --client-certificate=kube-proxy.pem \
  --client-key=kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes \
  --user=kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
</code></pre>
<ul>
<li><code>--embed-certs=true</code>：将 ca.pem 和 admin.pem 证书内容嵌入到生成的 kubectl-proxy.kubeconfig 文件中(不加时，写入的是证书文件路径)；</li>
</ul>
<p>分发 kubeconfig 文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_name in $&#123;NODE_NAMES[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_name&#125;&quot;
    scp kube-proxy.kubeconfig k8s@$&#123;node_name&#125;:/etc/kubernetes/
  done
</code></pre>
<h2 id="创建-kube-proxy-配置文件"><a href="#创建-kube-proxy-配置文件" class="headerlink" title="创建 kube-proxy 配置文件"></a>创建 kube-proxy 配置文件</h2><p>从 v1.10 开始，kube-proxy <strong>部分参数</strong>可以配置文件中配置。可以使用 <code>--write-config-to</code> 选项生成该配置文件，或者参考 kubeproxyconfig 的类型定义源文件 ：<a target="_blank" rel="noopener" href="https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/apis/kubeproxyconfig/types.go">https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/apis/kubeproxyconfig/types.go</a></p>
<p>创建 kube-proxy config 文件模板：</p>
<pre><code>cat &gt;kube-proxy.config.yaml.template &lt;&lt;EOF
apiVersion: kubeproxy.config.k8s.io/v1alpha1
bindAddress: ##NODE_IP##
clientConnection:
  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig
clusterCIDR: $&#123;CLUSTER_CIDR&#125;
healthzBindAddress: ##NODE_IP##:10256
hostnameOverride: ##NODE_NAME##
kind: KubeProxyConfiguration
metricsBindAddress: ##NODE_IP##:10249
mode: &quot;ipvs&quot;
EOF
</code></pre>
<ul>
<li><code>bindAddress</code>: 监听地址；</li>
<li><code>clientConnection.kubeconfig</code>: 连接 apiserver 的 kubeconfig 文件；</li>
<li><code>clusterCIDR</code>: kube-proxy 根据 <code>--cluster-cidr</code> 判断集群内部和外部流量，指定 <code>--cluster-cidr</code> 或 <code>--masquerade-all</code> 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT；</li>
<li><code>hostnameOverride</code>: 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 ipvs 规则；</li>
<li><code>mode</code>: 使用 ipvs 模式；</li>
</ul>
<p>为各节点创建和分发 kube-proxy 配置文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for (( i=0; i &lt; 7; i++ ))
  do 
    echo &quot;&gt;&gt;&gt; $&#123;NODE_NAMES[i]&#125;&quot;
    sed -e &quot;s/##NODE_NAME##/$&#123;NODE_NAMES[i]&#125;/&quot; -e &quot;s/##NODE_IP##/$&#123;NODE_IPS[i]&#125;/&quot; kube-proxy.config.yaml.template &gt; kube-proxy-$&#123;NODE_NAMES[i]&#125;.config.yaml
    scp kube-proxy-$&#123;NODE_NAMES[i]&#125;.config.yaml root@$&#123;NODE_NAMES[i]&#125;:/etc/kubernetes/kube-proxy.config.yaml
  done
</code></pre>
<p>替换后的 kube-proxy.config.yaml 文件：<a target="_blank" rel="noopener" href="https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/systemd/kube-proxy.config.yaml">kube-proxy.config.yaml</a></p>
<h2 id="创建和分发-kube-proxy-systemd-unit-文件"><a href="#创建和分发-kube-proxy-systemd-unit-文件" class="headerlink" title="创建和分发 kube-proxy systemd unit 文件"></a>创建和分发 kube-proxy systemd unit 文件</h2><pre><code>source /opt/k8s/bin/environment.sh
cat &gt; kube-proxy.service &lt;&lt;EOF
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
WorkingDirectory=/var/lib/kube-proxy
ExecStart=/opt/k8s/bin/kube-proxy \\
  --config=/etc/kubernetes/kube-proxy.config.yaml \\
  --alsologtostderr=true \\
  --logtostderr=false \\
  --log-dir=/var/log/kubernetes \\
  --v=2
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<p>替换后的 unit 文件：<a target="_blank" rel="noopener" href="https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/systemd/kube-proxy.service">kube-proxy.service</a></p>
<p>分发 kube-proxy systemd unit 文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_name in $&#123;NODE_NAMES[@]&#125;
  do 
    echo &quot;&gt;&gt;&gt; $&#123;node_name&#125;&quot;
    scp kube-proxy.service root@$&#123;node_name&#125;:/etc/systemd/system/
  done
</code></pre>
<h3 id="启动-kube-proxy-服务"><a href="#启动-kube-proxy-服务" class="headerlink" title="启动 kube-proxy 服务"></a>启动 kube-proxy 服务</h3><pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    ssh root@$&#123;node_ip&#125; &quot;mkdir -p /var/lib/kube-proxy&quot;
    ssh root@$&#123;node_ip&#125; &quot;mkdir -p /var/log/kubernetes &amp;&amp; chown -R k8s /var/log/kubernetes&quot;
    ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-proxy &amp;&amp; systemctl restart kube-proxy&quot;
  done
</code></pre>
<ul>
<li>必须先创建工作和日志目录；</li>
</ul>
<h3 id="检查启动结果-2"><a href="#检查启动结果-2" class="headerlink" title="检查启动结果"></a>检查启动结果</h3><pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    ssh k8s@$&#123;node_ip&#125; &quot;systemctl status kube-proxy|grep Active&quot;
  done
</code></pre>
<p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p>
<pre><code>journalctl -u kube-proxy
</code></pre>
<h2 id="查看监听端口和-metrics"><a href="#查看监听端口和-metrics" class="headerlink" title="查看监听端口和 metrics"></a>查看监听端口和 metrics</h2><pre><code>[k8s@kube-node1 ~]$ sudo netstat -lnpt|grep kube-prox
tcp        0      0 172.27.129.105:10249    0.0.0.0:*               LISTEN      16847/kube-proxy
tcp        0      0 172.27.129.105:10256    0.0.0.0:*               LISTEN      16847/kube-proxy
</code></pre>
<ul>
<li>10249：http prometheus metrics port;</li>
<li>10256：http healthz port;</li>
</ul>
<h2 id="查看-ipvs-路由规则"><a href="#查看-ipvs-路由规则" class="headerlink" title="查看 ipvs 路由规则"></a>查看 ipvs 路由规则</h2><pre><code>source /opt/k8s/bin/environment.sh
for node_ip in $&#123;NODE_IPS[@]&#125;
  do
    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;
    ssh root@$&#123;node_ip&#125; &quot;/usr/sbin/ipvsadm -ln&quot;
  done
</code></pre>
<p>预期输出：</p>
<pre><code>&gt;&gt;&gt; 172.27.129.105
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.0.1:443 rr persistent 10800
  -&gt; 172.27.129.105:6443          Masq    1      0          0
&gt;&gt;&gt; 172.27.129.111
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.0.1:443 rr persistent 10800
  -&gt; 172.27.129.105:6443          Masq    1      0          0
&gt;&gt;&gt; 172.27.129.112
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.0.1:443 rr persistent 10800
  -&gt; 172.27.129.105:6443          Masq    1      0          0
</code></pre>
<p>可见将所有到 kubernetes cluster ip 443 端口的请求都转发到 kube-apiserver 的 6443 端口；</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/jadepeng/tags/jqpeng/" rel="tag"># jqpeng</a>
          </div>
        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/jadepeng/2018/09/28/jqpeng-XNginx%20%20-%20nginx%20%E9%9B%86%E7%BE%A4%E5%8F%AF%E8%A7%86%E5%8C%96%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7/" rel="prev" title="XNginx  - nginx 集群可视化管理工具">
      <i class="fa fa-chevron-left"></i> XNginx  - nginx 集群可视化管理工具
    </a></div>
      <div class="post-nav-item">
    <a href="/jadepeng/2018/11/27/jqpeng-%E6%98%93%E4%BC%81%E7%A7%80H5%20json%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%A7%A3%E5%AF%86%E5%88%86%E6%9E%90/" rel="next" title="易企秀H5 json配置文件解密分析">
      易企秀H5 json配置文件解密分析 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#01-%E7%B3%BB%E7%BB%9F%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E5%85%A8%E5%B1%80%E5%8F%98%E9%87%8F"><span class="nav-number">1.</span> <span class="nav-text">01.系统初始化和全局变量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%BB%E5%8A%A0-k8s-%E5%92%8C-docker-%E8%B4%A6%E6%88%B7"><span class="nav-number">1.1.</span> <span class="nav-text">添加 k8s 和 docker 账户</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%A0%E5%AF%86%E7%A0%81-ssh-%E7%99%BB%E5%BD%95%E5%85%B6%E5%AE%83%E8%8A%82%E7%82%B9"><span class="nav-number">1.2.</span> <span class="nav-text">无密码 ssh 登录其它节点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E5%85%A8%E5%B1%80%E5%8F%98%E9%87%8F"><span class="nav-number">1.3.</span> <span class="nav-text">定义全局变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CA%E8%AF%81%E4%B9%A6"><span class="nav-number">1.4.</span> <span class="nav-text">CA证书</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%94%9F%E6%88%90-CA-%E8%AF%81%E4%B9%A6%E5%92%8C%E7%A7%81%E9%92%A5"><span class="nav-number">1.4.1.</span> <span class="nav-text">生成 CA 证书和私钥</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%89%E8%A3%85"><span class="nav-number">1.5.</span> <span class="nav-text">客户端安装</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA-admin-%E8%AF%81%E4%B9%A6%E5%92%8C%E7%A7%81%E9%92%A5"><span class="nav-number">1.6.</span> <span class="nav-text">创建 admin 证书和私钥</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E8%AF%81%E4%B9%A6%E5%92%8C%E7%A7%81%E9%92%A5%EF%BC%9A"><span class="nav-number">1.7.</span> <span class="nav-text">生成证书和私钥：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA-kubeconfig-%E6%96%87%E4%BB%B6"><span class="nav-number">1.8.</span> <span class="nav-text">创建 kubeconfig 文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E5%8F%91-kubeconfig-%E6%96%87%E4%BB%B6"><span class="nav-number">1.9.</span> <span class="nav-text">分发 kubeconfig 文件</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#etcd%E5%AE%89%E8%A3%85"><span class="nav-number">2.</span> <span class="nav-text">etcd安装</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E5%8F%91%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%87%E4%BB%B6%E5%88%B0%E9%9B%86%E7%BE%A4%E6%89%80%E6%9C%89%E8%8A%82%E7%82%B9%EF%BC%9A"><span class="nav-number">2.1.</span> <span class="nav-text">分发二进制文件到集群所有节点：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA-etcd-%E8%AF%81%E4%B9%A6%E5%92%8C%E7%A7%81%E9%92%A5"><span class="nav-number">2.2.</span> <span class="nav-text">创建 etcd 证书和私钥</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E8%AF%81%E4%B9%A6%E5%92%8C%E7%A7%81%E9%92%A5%EF%BC%9A-1"><span class="nav-number">2.3.</span> <span class="nav-text">生成证书和私钥：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E5%8F%91%E7%94%9F%E6%88%90%E7%9A%84%E8%AF%81%E4%B9%A6%E5%92%8C%E7%A7%81%E9%92%A5%E5%88%B0%E5%90%84-etcd-%E8%8A%82%E7%82%B9%EF%BC%9A"><span class="nav-number">2.4.</span> <span class="nav-text">分发生成的证书和私钥到各 etcd 节点：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA-etcd-%E7%9A%84-systemd-unit-%E6%A8%A1%E6%9D%BF%E6%96%87%E4%BB%B6"><span class="nav-number">2.5.</span> <span class="nav-text">创建 etcd 的 systemd unit 模板文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E5%90%84%E8%8A%82%E7%82%B9%E5%88%9B%E5%BB%BA%E5%92%8C%E5%88%86%E5%8F%91-etcd-systemd-unit-%E6%96%87%E4%BB%B6"><span class="nav-number">2.6.</span> <span class="nav-text">为各节点创建和分发 etcd systemd unit 文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8-etcd-%E6%9C%8D%E5%8A%A1"><span class="nav-number">2.7.</span> <span class="nav-text">启动 etcd 服务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A3%80%E6%9F%A5%E5%90%AF%E5%8A%A8%E7%BB%93%E6%9E%9C"><span class="nav-number">2.8.</span> <span class="nav-text">检查启动结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AA%8C%E8%AF%81%E6%9C%8D%E5%8A%A1%E7%8A%B6%E6%80%81"><span class="nav-number">2.9.</span> <span class="nav-text">验证服务状态</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#05-%E9%83%A8%E7%BD%B2-flannel-%E7%BD%91%E7%BB%9C"><span class="nav-number">3.</span> <span class="nav-text">05.部署 flannel 网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8B%E8%BD%BD%E5%92%8C%E5%88%86%E5%8F%91-flanneld-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%87%E4%BB%B6"><span class="nav-number">3.1.</span> <span class="nav-text">下载和分发 flanneld 二进制文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E8%AF%81%E4%B9%A6%E7%AD%BE%E5%90%8D%E8%AF%B7%E6%B1%82%EF%BC%9A"><span class="nav-number">3.2.</span> <span class="nav-text">创建证书签名请求：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA-flanneld-%E7%9A%84-systemd-unit-%E6%96%87%E4%BB%B6"><span class="nav-number">3.3.</span> <span class="nav-text">创建 flanneld 的 systemd unit 文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E5%8F%91-flanneld-systemd-unit-%E6%96%87%E4%BB%B6%E5%88%B0%E6%89%80%E6%9C%89%E8%8A%82%E7%82%B9"><span class="nav-number">3.4.</span> <span class="nav-text">分发 flanneld systemd unit 文件到所有节点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8-flanneld-%E6%9C%8D%E5%8A%A1"><span class="nav-number">3.5.</span> <span class="nav-text">启动 flanneld 服务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A3%80%E6%9F%A5%E5%90%AF%E5%8A%A8%E7%BB%93%E6%9E%9C-1"><span class="nav-number">3.6.</span> <span class="nav-text">检查启动结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A3%80%E6%9F%A5%E5%88%86%E9%85%8D%E7%BB%99%E5%90%84-flanneld-%E7%9A%84-Pod-%E7%BD%91%E6%AE%B5%E4%BF%A1%E6%81%AF"><span class="nav-number">3.7.</span> <span class="nav-text">检查分配给各 flanneld 的 Pod 网段信息</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#06-0-%E9%83%A8%E7%BD%B2-master-%E8%8A%82%E7%82%B9"><span class="nav-number">4.</span> <span class="nav-text">06-0.部署 master 节点</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8B%E8%BD%BD%E6%9C%80%E6%96%B0%E7%89%88%E6%9C%AC%E7%9A%84%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%87%E4%BB%B6"><span class="nav-number">4.1.</span> <span class="nav-text">下载最新版本的二进制文件</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#06-1-%E9%83%A8%E7%BD%B2%E9%AB%98%E5%8F%AF%E7%94%A8%E7%BB%84%E4%BB%B6%EF%BC%88keepalived-haproxy"><span class="nav-number">5.</span> <span class="nav-text">06-1.部署高可用组件（keepalived+haproxy)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6%E5%8C%85"><span class="nav-number">5.1.</span> <span class="nav-text">安装软件包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E5%92%8C%E4%B8%8B%E5%8F%91-haproxy-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="nav-number">5.2.</span> <span class="nav-text">配置和下发 haproxy 配置文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E5%92%8C%E4%B8%8B%E5%8F%91-keepalived-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="nav-number">5.3.</span> <span class="nav-text">配置和下发 keepalived 配置文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8B%E5%8F%91-keepalived-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="nav-number">5.4.</span> <span class="nav-text">下发 keepalived 配置文件</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#06-1-%E9%83%A8%E7%BD%B2-kube-apiserver-%E7%BB%84%E4%BB%B6"><span class="nav-number">6.</span> <span class="nav-text">06-1.部署 kube-apiserver 组件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA-kubernetes-%E8%AF%81%E4%B9%A6%E5%92%8C%E7%A7%81%E9%92%A5"><span class="nav-number">6.1.</span> <span class="nav-text">创建 kubernetes 证书和私钥</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E5%8A%A0%E5%AF%86%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="nav-number">6.2.</span> <span class="nav-text">创建加密配置文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA-kube-apiserver-systemd-unit-%E6%A8%A1%E6%9D%BF%E6%96%87%E4%BB%B6"><span class="nav-number">6.3.</span> <span class="nav-text">创建 kube-apiserver systemd unit 模板文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E5%90%84%E8%8A%82%E7%82%B9%E5%88%9B%E5%BB%BA%E5%92%8C%E5%88%86%E5%8F%91-kube-apiserver-systemd-unit-%E6%96%87%E4%BB%B6"><span class="nav-number">6.4.</span> <span class="nav-text">为各节点创建和分发 kube-apiserver systemd unit 文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8-kube-apiserver-%E6%9C%8D%E5%8A%A1"><span class="nav-number">6.5.</span> <span class="nav-text">启动 kube-apiserver 服务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A3%80%E6%9F%A5-kube-apiserver-%E8%BF%90%E8%A1%8C%E7%8A%B6%E6%80%81"><span class="nav-number">6.6.</span> <span class="nav-text">检查 kube-apiserver 运行状态</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%89%93%E5%8D%B0-kube-apiserver-%E5%86%99%E5%85%A5-etcd-%E7%9A%84%E6%95%B0%E6%8D%AE"><span class="nav-number">6.7.</span> <span class="nav-text">打印 kube-apiserver 写入 etcd 的数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%88%E4%BA%88-kubernetes-%E8%AF%81%E4%B9%A6%E8%AE%BF%E9%97%AE-kubelet-API-%E7%9A%84%E6%9D%83%E9%99%90"><span class="nav-number">6.8.</span> <span class="nav-text">授予 kubernetes 证书访问 kubelet API 的权限</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#06-3-%E9%83%A8%E7%BD%B2%E9%AB%98%E5%8F%AF%E7%94%A8-kube-controller-manager-%E9%9B%86%E7%BE%A4"><span class="nav-number">7.</span> <span class="nav-text">06-3.部署高可用 kube-controller-manager 集群</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA-kube-controller-manager-%E8%AF%81%E4%B9%A6%E5%92%8C%E7%A7%81%E9%92%A5"><span class="nav-number">7.1.</span> <span class="nav-text">创建 kube-controller-manager 证书和私钥</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E5%92%8C%E5%88%86%E5%8F%91-kubeconfig-%E6%96%87%E4%BB%B6"><span class="nav-number">7.2.</span> <span class="nav-text">创建和分发 kubeconfig 文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E5%92%8C%E5%88%86%E5%8F%91-kube-controller-manager-systemd-unit-%E6%96%87%E4%BB%B6"><span class="nav-number">7.3.</span> <span class="nav-text">创建和分发 kube-controller-manager systemd unit 文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kube-controller-manager-%E7%9A%84%E6%9D%83%E9%99%90"><span class="nav-number">7.4.</span> <span class="nav-text">kube-controller-manager 的权限</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8-kube-controller-manager-%E6%9C%8D%E5%8A%A1"><span class="nav-number">7.5.</span> <span class="nav-text">启动 kube-controller-manager 服务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A3%80%E6%9F%A5%E6%9C%8D%E5%8A%A1%E8%BF%90%E8%A1%8C%E7%8A%B6%E6%80%81"><span class="nav-number">7.6.</span> <span class="nav-text">检查服务运行状态</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9F%A5%E7%9C%8B%E8%BE%93%E5%87%BA%E7%9A%84-metric"><span class="nav-number">7.7.</span> <span class="nav-text">查看输出的 metric</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9F%A5%E7%9C%8B%E5%BD%93%E5%89%8D%E7%9A%84-leader"><span class="nav-number">7.8.</span> <span class="nav-text">查看当前的 leader</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#06-3-%E9%83%A8%E7%BD%B2%E9%AB%98%E5%8F%AF%E7%94%A8-kube-scheduler-%E9%9B%86%E7%BE%A4"><span class="nav-number">8.</span> <span class="nav-text">06-3.部署高可用 kube-scheduler 集群</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA-kube-scheduler-%E8%AF%81%E4%B9%A6%E5%92%8C%E7%A7%81%E9%92%A5"><span class="nav-number">8.1.</span> <span class="nav-text">创建 kube-scheduler 证书和私钥</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E5%92%8C%E5%88%86%E5%8F%91-kubeconfig-%E6%96%87%E4%BB%B6-1"><span class="nav-number">8.2.</span> <span class="nav-text">创建和分发 kubeconfig 文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E5%92%8C%E5%88%86%E5%8F%91-kube-scheduler-systemd-unit-%E6%96%87%E4%BB%B6"><span class="nav-number">8.3.</span> <span class="nav-text">创建和分发 kube-scheduler systemd unit 文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E5%8F%91-systemd-unit-%E6%96%87%E4%BB%B6%E5%88%B0%E6%89%80%E6%9C%89-master-%E8%8A%82%E7%82%B9%EF%BC%9A"><span class="nav-number">8.4.</span> <span class="nav-text">分发 systemd unit 文件到所有 master 节点：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8-kube-scheduler-%E6%9C%8D%E5%8A%A1"><span class="nav-number">8.5.</span> <span class="nav-text">启动 kube-scheduler 服务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9F%A5%E7%9C%8B%E8%BE%93%E5%87%BA%E7%9A%84-metric-1"><span class="nav-number">8.6.</span> <span class="nav-text">查看输出的 metric</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#HELP-apiserver-audit-event-total-Counter-of-audit-events-generated-and-sent-to-the-audit-backend"><span class="nav-number"></span> <span class="nav-text">HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TYPE-apiserver-audit-event-total-counter"><span class="nav-number"></span> <span class="nav-text">TYPE apiserver_audit_event_total counter</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#HELP-go-gc-duration-seconds-A-summary-of-the-GC-invocation-durations"><span class="nav-number"></span> <span class="nav-text">HELP go_gc_duration_seconds A summary of the GC invocation durations.</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TYPE-go-gc-duration-seconds-summary"><span class="nav-number"></span> <span class="nav-text">TYPE go_gc_duration_seconds summary</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B5%8B%E8%AF%95-kube-scheduler-%E9%9B%86%E7%BE%A4%E7%9A%84%E9%AB%98%E5%8F%AF%E7%94%A8"><span class="nav-number">0.1.</span> <span class="nav-text">测试 kube-scheduler 集群的高可用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#07-1-%E9%83%A8%E7%BD%B2-docker-%E7%BB%84%E4%BB%B6"><span class="nav-number">1.</span> <span class="nav-text">07-1.部署 docker 组件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96%E5%8C%85"><span class="nav-number">1.1.</span> <span class="nav-text">安装依赖包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8B%E8%BD%BD%E5%92%8C%E5%88%86%E5%8F%91-docker-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%87%E4%BB%B6"><span class="nav-number">1.2.</span> <span class="nav-text">下载和分发 docker 二进制文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E5%92%8C%E5%88%86%E5%8F%91-systemd-unit-%E6%96%87%E4%BB%B6"><span class="nav-number">1.3.</span> <span class="nav-text">创建和分发 systemd unit 文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E5%92%8C%E5%88%86%E5%8F%91-docker-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="nav-number">1.4.</span> <span class="nav-text">配置和分发 docker 配置文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8-docker-%E6%9C%8D%E5%8A%A1"><span class="nav-number">1.5.</span> <span class="nav-text">启动 docker 服务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A3%80%E6%9F%A5%E6%9C%8D%E5%8A%A1%E8%BF%90%E8%A1%8C%E7%8A%B6%E6%80%81-1"><span class="nav-number">1.6.</span> <span class="nav-text">检查服务运行状态</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A3%80%E6%9F%A5-docker0-%E7%BD%91%E6%A1%A5"><span class="nav-number">1.6.1.</span> <span class="nav-text">检查 docker0 网桥</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#07-2-%E9%83%A8%E7%BD%B2-kubelet-%E7%BB%84%E4%BB%B6"><span class="nav-number">2.</span> <span class="nav-text">07-2.部署 kubelet 组件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA-kubelet-bootstrap-kubeconfig-%E6%96%87%E4%BB%B6"><span class="nav-number">2.1.</span> <span class="nav-text">创建 kubelet bootstrap kubeconfig 文件</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E5%8F%91-bootstrap-kubeconfig-%E6%96%87%E4%BB%B6%E5%88%B0%E6%89%80%E6%9C%89-worker-%E8%8A%82%E7%82%B9"><span class="nav-number">3.</span> <span class="nav-text">分发 bootstrap kubeconfig 文件到所有 worker 节点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E5%92%8C%E5%88%86%E5%8F%91-kubelet-%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="nav-number">4.</span> <span class="nav-text">创建和分发 kubelet 参数配置文件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E5%92%8C%E5%88%86%E5%8F%91-kubelet-systemd-unit-%E6%96%87%E4%BB%B6"><span class="nav-number">5.</span> <span class="nav-text">创建和分发 kubelet systemd unit 文件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bootstrap-Token-Auth-%E5%92%8C%E6%8E%88%E4%BA%88%E6%9D%83%E9%99%90"><span class="nav-number">6.</span> <span class="nav-text">Bootstrap Token Auth 和授予权限</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8-kubelet-%E6%9C%8D%E5%8A%A1"><span class="nav-number">7.</span> <span class="nav-text">启动 kubelet 服务</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#approve-kubelet-CSR-%E8%AF%B7%E6%B1%82"><span class="nav-number">8.</span> <span class="nav-text">approve kubelet CSR 请求</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%89%8B%E5%8A%A8-approve-CSR-%E8%AF%B7%E6%B1%82"><span class="nav-number">8.1.</span> <span class="nav-text">手动 approve CSR 请求</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8-approve-CSR-%E8%AF%B7%E6%B1%82"><span class="nav-number">8.2.</span> <span class="nav-text">自动 approve CSR 请求</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9F%A5%E7%9C%8B-kublet-%E7%9A%84%E6%83%85%E5%86%B5"><span class="nav-number">9.</span> <span class="nav-text">查看 kublet 的情况</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kubelet-%E6%8F%90%E4%BE%9B%E7%9A%84-API-%E6%8E%A5%E5%8F%A3"><span class="nav-number">10.</span> <span class="nav-text">kubelet 提供的 API 接口</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kublet-api-%E8%AE%A4%E8%AF%81%E5%92%8C%E6%8E%88%E6%9D%83"><span class="nav-number">11.</span> <span class="nav-text">kublet api 认证和授权</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#cadvisor-%E5%92%8C-metrics"><span class="nav-number">11.1.</span> <span class="nav-text">cadvisor 和 metrics</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%8E%B7%E5%8F%96-kublet-%E7%9A%84%E9%85%8D%E7%BD%AE"><span class="nav-number">12.</span> <span class="nav-text">获取 kublet 的配置</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">13.</span> <span class="nav-text">参考</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#07-3-%E9%83%A8%E7%BD%B2-kube-proxy-%E7%BB%84%E4%BB%B6"><span class="nav-number">14.</span> <span class="nav-text">07-3.部署 kube-proxy 组件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8B%E8%BD%BD%E5%92%8C%E5%88%86%E5%8F%91-kube-proxy-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%87%E4%BB%B6"><span class="nav-number">14.1.</span> <span class="nav-text">下载和分发 kube-proxy 二进制文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96%E5%8C%85-1"><span class="nav-number">14.2.</span> <span class="nav-text">安装依赖包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA-kube-proxy-%E8%AF%81%E4%B9%A6"><span class="nav-number">14.3.</span> <span class="nav-text">创建 kube-proxy 证书</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E5%92%8C%E5%88%86%E5%8F%91-kubeconfig-%E6%96%87%E4%BB%B6-2"><span class="nav-number">15.</span> <span class="nav-text">创建和分发 kubeconfig 文件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA-kube-proxy-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="nav-number">16.</span> <span class="nav-text">创建 kube-proxy 配置文件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E5%92%8C%E5%88%86%E5%8F%91-kube-proxy-systemd-unit-%E6%96%87%E4%BB%B6"><span class="nav-number">17.</span> <span class="nav-text">创建和分发 kube-proxy systemd unit 文件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8-kube-proxy-%E6%9C%8D%E5%8A%A1"><span class="nav-number">17.1.</span> <span class="nav-text">启动 kube-proxy 服务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A3%80%E6%9F%A5%E5%90%AF%E5%8A%A8%E7%BB%93%E6%9E%9C-2"><span class="nav-number">17.2.</span> <span class="nav-text">检查启动结果</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9F%A5%E7%9C%8B%E7%9B%91%E5%90%AC%E7%AB%AF%E5%8F%A3%E5%92%8C-metrics"><span class="nav-number">18.</span> <span class="nav-text">查看监听端口和 metrics</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9F%A5%E7%9C%8B-ipvs-%E8%B7%AF%E7%94%B1%E8%A7%84%E5%88%99"><span class="nav-number">19.</span> <span class="nav-text">查看 ipvs 路由规则</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="JadePeng"
      src="/jadepeng/images/avatar.gif">
  <p class="site-author-name" itemprop="name">JadePeng</p>
  <div class="site-description" itemprop="description">JadePeng的技术笔记本</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/jadepeng/archives/">
        
          <span class="site-state-item-count">98</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/jadepeng/categories/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/jadepeng/tags/">
          
        <span class="site-state-item-count">83</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">JadePeng</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">668k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">10:08</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/jadepeng/lib/anime.min.js"></script>
  <script src="/jadepeng/lib/velocity/velocity.min.js"></script>
  <script src="/jadepeng/lib/velocity/velocity.ui.min.js"></script>

<script src="/jadepeng/js/utils.js"></script>

<script src="/jadepeng/js/motion.js"></script>


<script src="/jadepeng/js/schemes/muse.js"></script>


<script src="/jadepeng/js/next-boot.js"></script>




  




  
<script src="/jadepeng/js/local-search.js"></script>









<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/jadepeng/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>


<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'forest',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  

</body>
</html>
